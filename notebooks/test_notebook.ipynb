{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb76d91",
   "metadata": {},
   "source": [
    "# Notebook for some tests\n",
    "This notebook will contain tests and some early results. I will keep some of the results on this notebook but mote likely will expand onto new more developed notebooks on the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0256b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ee4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.trainer import RLTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c22dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RLTrainer(\n",
    "    agent_type=\"PPO\",\n",
    "    xml_file=\"mujoco_sim/xml_models/pendulum_model_v2.xml\",\n",
    "    render_mode=None,\n",
    "    max_steps=3000,\n",
    "    seed=40,\n",
    "    policy=\"MlpPolicy\",\n",
    "    agent_kwargs={\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe3976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de resultados: results/run_5\n",
      "Entrenando PPO por 5000000 pasos.\n",
      "Eval num_timesteps=5000, episode_reward=-673.55 +/- 6.93\n",
      "Episode length: 199.20 +/- 18.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-673.01 +/- 6.56\n",
      "Episode length: 223.80 +/- 38.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-677.37 +/- 5.51\n",
      "Episode length: 206.40 +/- 22.55\n",
      "Eval num_timesteps=20000, episode_reward=-673.94 +/- 4.50\n",
      "Episode length: 252.80 +/- 51.58\n",
      "Eval num_timesteps=25000, episode_reward=-673.21 +/- 4.75\n",
      "Episode length: 271.00 +/- 40.65\n",
      "Eval num_timesteps=30000, episode_reward=-668.80 +/- 4.87\n",
      "Episode length: 257.40 +/- 14.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-671.24 +/- 3.40\n",
      "Episode length: 242.60 +/- 30.31\n",
      "Eval num_timesteps=40000, episode_reward=-660.93 +/- 6.31\n",
      "Episode length: 294.80 +/- 47.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-658.16 +/- 3.57\n",
      "Episode length: 275.80 +/- 67.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-665.43 +/- 4.76\n",
      "Episode length: 212.00 +/- 37.93\n",
      "Eval num_timesteps=55000, episode_reward=-672.76 +/- 5.38\n",
      "Episode length: 268.60 +/- 111.08\n",
      "Eval num_timesteps=60000, episode_reward=-675.37 +/- 9.04\n",
      "Episode length: 222.20 +/- 57.31\n",
      "Eval num_timesteps=65000, episode_reward=-665.16 +/- 11.56\n",
      "Episode length: 232.60 +/- 75.51\n",
      "Eval num_timesteps=70000, episode_reward=-668.88 +/- 6.83\n",
      "Episode length: 210.80 +/- 40.03\n",
      "Eval num_timesteps=75000, episode_reward=-665.89 +/- 5.08\n",
      "Episode length: 255.00 +/- 65.92\n",
      "Eval num_timesteps=80000, episode_reward=-663.53 +/- 6.98\n",
      "Episode length: 309.40 +/- 58.32\n",
      "Eval num_timesteps=85000, episode_reward=-666.19 +/- 5.11\n",
      "Episode length: 233.60 +/- 30.33\n",
      "Eval num_timesteps=90000, episode_reward=-660.15 +/- 5.20\n",
      "Episode length: 309.00 +/- 162.11\n",
      "Eval num_timesteps=95000, episode_reward=-660.59 +/- 3.60\n",
      "Episode length: 254.20 +/- 49.06\n",
      "Eval num_timesteps=100000, episode_reward=-649.28 +/- 6.56\n",
      "Episode length: 280.40 +/- 34.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=-647.53 +/- 5.07\n",
      "Episode length: 289.60 +/- 41.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-648.62 +/- 6.37\n",
      "Episode length: 275.00 +/- 42.55\n",
      "Eval num_timesteps=115000, episode_reward=-646.41 +/- 5.26\n",
      "Episode length: 264.00 +/- 31.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-645.22 +/- 4.33\n",
      "Episode length: 248.40 +/- 28.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=-642.70 +/- 5.80\n",
      "Episode length: 244.20 +/- 22.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-646.56 +/- 5.69\n",
      "Episode length: 350.80 +/- 116.20\n",
      "Eval num_timesteps=135000, episode_reward=-647.60 +/- 6.22\n",
      "Episode length: 296.40 +/- 53.89\n",
      "Eval num_timesteps=140000, episode_reward=-652.92 +/- 4.07\n",
      "Episode length: 274.00 +/- 35.34\n",
      "Eval num_timesteps=145000, episode_reward=-648.11 +/- 2.54\n",
      "Episode length: 305.80 +/- 34.43\n",
      "Eval num_timesteps=150000, episode_reward=-634.64 +/- 6.20\n",
      "Episode length: 309.40 +/- 81.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=155000, episode_reward=-643.67 +/- 4.67\n",
      "Episode length: 341.40 +/- 111.85\n",
      "Eval num_timesteps=160000, episode_reward=-640.82 +/- 3.92\n",
      "Episode length: 463.40 +/- 96.15\n",
      "Eval num_timesteps=165000, episode_reward=-635.78 +/- 4.04\n",
      "Episode length: 367.00 +/- 115.69\n",
      "Eval num_timesteps=170000, episode_reward=-640.33 +/- 5.94\n",
      "Episode length: 312.20 +/- 94.08\n",
      "Eval num_timesteps=175000, episode_reward=-637.64 +/- 2.71\n",
      "Episode length: 264.20 +/- 27.39\n",
      "Eval num_timesteps=180000, episode_reward=-642.61 +/- 5.61\n",
      "Episode length: 268.00 +/- 21.25\n",
      "Eval num_timesteps=185000, episode_reward=-644.10 +/- 4.99\n",
      "Episode length: 323.40 +/- 44.87\n",
      "Eval num_timesteps=190000, episode_reward=-643.34 +/- 4.79\n",
      "Episode length: 303.60 +/- 54.62\n",
      "Eval num_timesteps=195000, episode_reward=-641.71 +/- 6.28\n",
      "Episode length: 261.20 +/- 47.91\n",
      "Eval num_timesteps=200000, episode_reward=-641.90 +/- 5.67\n",
      "Episode length: 374.60 +/- 92.59\n",
      "Eval num_timesteps=205000, episode_reward=-644.95 +/- 6.24\n",
      "Episode length: 272.80 +/- 45.44\n",
      "Eval num_timesteps=210000, episode_reward=-642.46 +/- 4.18\n",
      "Episode length: 293.20 +/- 37.09\n",
      "Eval num_timesteps=215000, episode_reward=-633.61 +/- 2.04\n",
      "Episode length: 260.00 +/- 22.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=-642.27 +/- 5.84\n",
      "Episode length: 308.00 +/- 33.91\n",
      "Eval num_timesteps=225000, episode_reward=-642.97 +/- 6.06\n",
      "Episode length: 244.00 +/- 29.29\n",
      "Eval num_timesteps=230000, episode_reward=-644.45 +/- 2.82\n",
      "Episode length: 365.60 +/- 130.52\n",
      "Eval num_timesteps=235000, episode_reward=-646.49 +/- 4.84\n",
      "Episode length: 276.80 +/- 52.35\n",
      "Eval num_timesteps=240000, episode_reward=-644.00 +/- 4.90\n",
      "Episode length: 255.00 +/- 53.01\n",
      "Eval num_timesteps=245000, episode_reward=-641.78 +/- 5.79\n",
      "Episode length: 292.00 +/- 53.16\n",
      "Eval num_timesteps=250000, episode_reward=-645.55 +/- 9.75\n",
      "Episode length: 330.80 +/- 114.61\n",
      "Eval num_timesteps=255000, episode_reward=-640.12 +/- 1.96\n",
      "Episode length: 349.40 +/- 134.83\n",
      "Eval num_timesteps=260000, episode_reward=-639.69 +/- 7.28\n",
      "Episode length: 296.60 +/- 100.90\n",
      "Eval num_timesteps=265000, episode_reward=-633.54 +/- 3.80\n",
      "Episode length: 321.00 +/- 84.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270000, episode_reward=-640.13 +/- 4.98\n",
      "Episode length: 269.40 +/- 44.37\n",
      "Eval num_timesteps=275000, episode_reward=-637.63 +/- 5.83\n",
      "Episode length: 289.80 +/- 63.59\n",
      "Eval num_timesteps=280000, episode_reward=-638.23 +/- 5.89\n",
      "Episode length: 310.00 +/- 68.18\n",
      "Eval num_timesteps=285000, episode_reward=-637.28 +/- 3.79\n",
      "Episode length: 270.20 +/- 48.95\n",
      "Eval num_timesteps=290000, episode_reward=-627.05 +/- 4.29\n",
      "Episode length: 268.80 +/- 52.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=295000, episode_reward=-633.10 +/- 2.64\n",
      "Episode length: 275.60 +/- 32.84\n",
      "Eval num_timesteps=300000, episode_reward=-637.31 +/- 4.34\n",
      "Episode length: 255.60 +/- 64.68\n",
      "Eval num_timesteps=305000, episode_reward=-640.85 +/- 5.03\n",
      "Episode length: 244.60 +/- 33.07\n",
      "Eval num_timesteps=310000, episode_reward=-639.52 +/- 6.88\n",
      "Episode length: 223.20 +/- 16.05\n",
      "Eval num_timesteps=315000, episode_reward=-637.61 +/- 7.51\n",
      "Episode length: 257.60 +/- 29.82\n",
      "Eval num_timesteps=320000, episode_reward=-636.48 +/- 3.79\n",
      "Episode length: 268.00 +/- 11.61\n",
      "Eval num_timesteps=325000, episode_reward=-634.54 +/- 4.53\n",
      "Episode length: 231.20 +/- 20.37\n",
      "Eval num_timesteps=330000, episode_reward=-638.91 +/- 6.85\n",
      "Episode length: 270.00 +/- 50.90\n",
      "Eval num_timesteps=335000, episode_reward=-638.33 +/- 4.62\n",
      "Episode length: 296.80 +/- 50.94\n",
      "Eval num_timesteps=340000, episode_reward=-636.54 +/- 3.34\n",
      "Episode length: 306.40 +/- 45.02\n",
      "Eval num_timesteps=345000, episode_reward=-630.43 +/- 4.49\n",
      "Episode length: 308.40 +/- 20.81\n",
      "Eval num_timesteps=350000, episode_reward=-635.09 +/- 4.49\n",
      "Episode length: 316.20 +/- 46.61\n",
      "Eval num_timesteps=355000, episode_reward=-634.57 +/- 4.79\n",
      "Episode length: 337.20 +/- 59.73\n",
      "Eval num_timesteps=360000, episode_reward=-633.17 +/- 3.54\n",
      "Episode length: 276.00 +/- 30.19\n",
      "Eval num_timesteps=365000, episode_reward=-634.86 +/- 2.47\n",
      "Episode length: 290.20 +/- 47.49\n",
      "Eval num_timesteps=370000, episode_reward=-636.36 +/- 3.59\n",
      "Episode length: 285.00 +/- 46.48\n",
      "Eval num_timesteps=375000, episode_reward=-633.37 +/- 4.00\n",
      "Episode length: 231.00 +/- 24.59\n",
      "Eval num_timesteps=380000, episode_reward=-637.72 +/- 2.93\n",
      "Episode length: 283.40 +/- 37.15\n",
      "Eval num_timesteps=385000, episode_reward=-639.68 +/- 4.35\n",
      "Episode length: 296.00 +/- 34.12\n",
      "Eval num_timesteps=390000, episode_reward=-638.45 +/- 4.51\n",
      "Episode length: 279.60 +/- 32.80\n",
      "Eval num_timesteps=395000, episode_reward=-635.36 +/- 4.35\n",
      "Episode length: 275.20 +/- 33.78\n",
      "Eval num_timesteps=400000, episode_reward=-640.10 +/- 3.52\n",
      "Episode length: 320.80 +/- 31.96\n",
      "Eval num_timesteps=405000, episode_reward=-643.05 +/- 5.47\n",
      "Episode length: 291.60 +/- 59.72\n",
      "Eval num_timesteps=410000, episode_reward=-645.40 +/- 5.56\n",
      "Episode length: 294.20 +/- 39.06\n",
      "Eval num_timesteps=415000, episode_reward=-642.05 +/- 1.60\n",
      "Episode length: 295.80 +/- 32.52\n",
      "Eval num_timesteps=420000, episode_reward=-649.74 +/- 4.25\n",
      "Episode length: 309.40 +/- 48.68\n",
      "Eval num_timesteps=425000, episode_reward=-655.16 +/- 5.39\n",
      "Episode length: 304.20 +/- 28.58\n",
      "Eval num_timesteps=430000, episode_reward=-650.05 +/- 5.44\n",
      "Episode length: 350.60 +/- 33.81\n",
      "Eval num_timesteps=435000, episode_reward=-656.18 +/- 2.61\n",
      "Episode length: 315.00 +/- 40.32\n",
      "Eval num_timesteps=440000, episode_reward=-653.02 +/- 2.83\n",
      "Episode length: 327.60 +/- 37.57\n",
      "Eval num_timesteps=445000, episode_reward=-662.92 +/- 7.07\n",
      "Episode length: 331.40 +/- 24.59\n",
      "Eval num_timesteps=450000, episode_reward=-669.86 +/- 3.55\n",
      "Episode length: 376.20 +/- 47.61\n",
      "Eval num_timesteps=455000, episode_reward=-676.22 +/- 2.77\n",
      "Episode length: 383.20 +/- 34.04\n",
      "Eval num_timesteps=460000, episode_reward=-677.92 +/- 5.59\n",
      "Episode length: 361.20 +/- 38.61\n",
      "Eval num_timesteps=465000, episode_reward=-704.93 +/- 4.66\n",
      "Episode length: 366.40 +/- 48.16\n",
      "Eval num_timesteps=470000, episode_reward=-714.12 +/- 10.72\n",
      "Episode length: 361.20 +/- 33.09\n",
      "Eval num_timesteps=475000, episode_reward=-679.46 +/- 4.86\n",
      "Episode length: 313.60 +/- 31.90\n",
      "Eval num_timesteps=480000, episode_reward=-682.86 +/- 3.62\n",
      "Episode length: 317.40 +/- 32.62\n",
      "Eval num_timesteps=485000, episode_reward=-688.38 +/- 6.10\n",
      "Episode length: 317.60 +/- 19.77\n",
      "Eval num_timesteps=490000, episode_reward=-673.87 +/- 3.92\n",
      "Episode length: 323.40 +/- 20.28\n",
      "Eval num_timesteps=495000, episode_reward=-663.94 +/- 6.05\n",
      "Episode length: 284.40 +/- 26.33\n",
      "Eval num_timesteps=500000, episode_reward=-670.45 +/- 3.28\n",
      "Episode length: 349.00 +/- 37.06\n",
      "Eval num_timesteps=505000, episode_reward=-665.74 +/- 7.22\n",
      "Episode length: 322.00 +/- 43.58\n",
      "Eval num_timesteps=510000, episode_reward=-662.45 +/- 5.57\n",
      "Episode length: 306.80 +/- 30.64\n",
      "Eval num_timesteps=515000, episode_reward=-669.59 +/- 6.26\n",
      "Episode length: 312.60 +/- 32.71\n",
      "Eval num_timesteps=520000, episode_reward=-681.71 +/- 2.96\n",
      "Episode length: 351.60 +/- 13.09\n",
      "Eval num_timesteps=525000, episode_reward=-718.43 +/- 3.71\n",
      "Episode length: 401.60 +/- 28.42\n",
      "Eval num_timesteps=530000, episode_reward=-705.85 +/- 5.35\n",
      "Episode length: 349.80 +/- 84.49\n",
      "Eval num_timesteps=535000, episode_reward=-735.38 +/- 4.78\n",
      "Episode length: 379.60 +/- 41.69\n",
      "Eval num_timesteps=540000, episode_reward=-702.30 +/- 19.11\n",
      "Episode length: 358.20 +/- 107.59\n",
      "Eval num_timesteps=545000, episode_reward=-729.11 +/- 16.28\n",
      "Episode length: 367.20 +/- 75.38\n",
      "Eval num_timesteps=550000, episode_reward=-714.72 +/- 16.20\n",
      "Episode length: 314.80 +/- 92.59\n",
      "Eval num_timesteps=555000, episode_reward=-695.34 +/- 7.46\n",
      "Episode length: 194.40 +/- 23.54\n",
      "Eval num_timesteps=560000, episode_reward=-741.37 +/- 57.45\n",
      "Episode length: 319.40 +/- 148.13\n",
      "Eval num_timesteps=565000, episode_reward=-779.47 +/- 14.56\n",
      "Episode length: 407.00 +/- 39.76\n",
      "Eval num_timesteps=570000, episode_reward=-769.22 +/- 10.70\n",
      "Episode length: 415.40 +/- 43.98\n",
      "Eval num_timesteps=575000, episode_reward=-799.17 +/- 14.92\n",
      "Episode length: 434.40 +/- 46.49\n",
      "Eval num_timesteps=580000, episode_reward=-784.57 +/- 13.00\n",
      "Episode length: 439.00 +/- 40.40\n",
      "Eval num_timesteps=585000, episode_reward=-813.76 +/- 18.94\n",
      "Episode length: 480.80 +/- 59.49\n",
      "Eval num_timesteps=590000, episode_reward=-833.43 +/- 21.82\n",
      "Episode length: 492.00 +/- 41.90\n",
      "Eval num_timesteps=595000, episode_reward=-722.90 +/- 65.99\n",
      "Episode length: 502.20 +/- 32.75\n",
      "Eval num_timesteps=600000, episode_reward=-820.58 +/- 18.22\n",
      "Episode length: 483.40 +/- 24.14\n",
      "Eval num_timesteps=605000, episode_reward=-743.56 +/- 7.79\n",
      "Episode length: 384.00 +/- 48.35\n",
      "Eval num_timesteps=610000, episode_reward=-750.76 +/- 13.38\n",
      "Episode length: 412.40 +/- 45.90\n",
      "Eval num_timesteps=615000, episode_reward=-815.45 +/- 9.33\n",
      "Episode length: 475.20 +/- 36.98\n",
      "Eval num_timesteps=620000, episode_reward=-856.85 +/- 93.53\n",
      "Episode length: 594.80 +/- 27.71\n",
      "Eval num_timesteps=625000, episode_reward=-734.05 +/- 55.34\n",
      "Episode length: 591.80 +/- 59.96\n",
      "Eval num_timesteps=630000, episode_reward=-673.57 +/- 10.05\n",
      "Episode length: 377.60 +/- 101.96\n",
      "Eval num_timesteps=635000, episode_reward=-680.13 +/- 10.79\n",
      "Episode length: 318.20 +/- 122.48\n",
      "Eval num_timesteps=640000, episode_reward=-682.06 +/- 5.59\n",
      "Episode length: 210.60 +/- 28.72\n",
      "Eval num_timesteps=645000, episode_reward=-665.86 +/- 7.23\n",
      "Episode length: 335.00 +/- 131.85\n",
      "Eval num_timesteps=650000, episode_reward=-684.88 +/- 5.76\n",
      "Episode length: 229.60 +/- 66.35\n",
      "Eval num_timesteps=655000, episode_reward=-677.60 +/- 5.27\n",
      "Episode length: 246.00 +/- 54.97\n",
      "Eval num_timesteps=660000, episode_reward=-670.26 +/- 5.44\n",
      "Episode length: 311.20 +/- 131.74\n",
      "Eval num_timesteps=665000, episode_reward=-673.97 +/- 4.91\n",
      "Episode length: 325.40 +/- 101.22\n",
      "Eval num_timesteps=670000, episode_reward=-665.60 +/- 7.89\n",
      "Episode length: 285.40 +/- 88.32\n",
      "Eval num_timesteps=675000, episode_reward=-679.35 +/- 5.48\n",
      "Episode length: 279.20 +/- 49.06\n",
      "Eval num_timesteps=680000, episode_reward=-683.66 +/- 3.44\n",
      "Episode length: 206.40 +/- 36.13\n",
      "Eval num_timesteps=685000, episode_reward=-670.87 +/- 8.59\n",
      "Episode length: 295.20 +/- 75.10\n",
      "Eval num_timesteps=690000, episode_reward=-668.56 +/- 5.53\n",
      "Episode length: 328.60 +/- 51.59\n",
      "Eval num_timesteps=695000, episode_reward=-669.26 +/- 4.84\n",
      "Episode length: 216.60 +/- 17.62\n",
      "Eval num_timesteps=700000, episode_reward=-656.72 +/- 4.72\n",
      "Episode length: 229.40 +/- 11.59\n",
      "Eval num_timesteps=705000, episode_reward=-652.87 +/- 2.81\n",
      "Episode length: 293.80 +/- 55.19\n",
      "Eval num_timesteps=710000, episode_reward=-648.67 +/- 2.52\n",
      "Episode length: 289.40 +/- 46.83\n",
      "Eval num_timesteps=715000, episode_reward=-678.08 +/- 54.41\n",
      "Episode length: 427.40 +/- 45.77\n",
      "Eval num_timesteps=720000, episode_reward=-615.19 +/- 103.67\n",
      "Episode length: 601.80 +/- 61.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=725000, episode_reward=-683.85 +/- 29.84\n",
      "Episode length: 731.60 +/- 62.46\n",
      "Eval num_timesteps=730000, episode_reward=-726.83 +/- 112.48\n",
      "Episode length: 1033.20 +/- 134.16\n",
      "Eval num_timesteps=735000, episode_reward=-655.59 +/- 29.59\n",
      "Episode length: 846.60 +/- 134.24\n",
      "Eval num_timesteps=740000, episode_reward=-628.85 +/- 62.02\n",
      "Episode length: 568.20 +/- 27.59\n",
      "Eval num_timesteps=745000, episode_reward=-736.66 +/- 159.41\n",
      "Episode length: 721.40 +/- 144.25\n",
      "Eval num_timesteps=750000, episode_reward=-721.46 +/- 85.71\n",
      "Episode length: 792.80 +/- 173.81\n",
      "Eval num_timesteps=755000, episode_reward=-748.49 +/- 83.36\n",
      "Episode length: 922.40 +/- 161.50\n",
      "Eval num_timesteps=760000, episode_reward=-755.00 +/- 125.28\n",
      "Episode length: 923.40 +/- 56.07\n",
      "Eval num_timesteps=765000, episode_reward=-741.13 +/- 57.09\n",
      "Episode length: 1354.20 +/- 233.97\n",
      "Eval num_timesteps=770000, episode_reward=-806.23 +/- 62.40\n",
      "Episode length: 969.40 +/- 83.70\n",
      "Eval num_timesteps=775000, episode_reward=-749.24 +/- 90.41\n",
      "Episode length: 1002.60 +/- 45.18\n",
      "Eval num_timesteps=780000, episode_reward=-703.51 +/- 32.67\n",
      "Episode length: 594.60 +/- 145.17\n",
      "Eval num_timesteps=785000, episode_reward=-746.34 +/- 74.82\n",
      "Episode length: 697.40 +/- 192.23\n",
      "Eval num_timesteps=790000, episode_reward=-676.76 +/- 13.56\n",
      "Episode length: 783.60 +/- 93.78\n",
      "Eval num_timesteps=795000, episode_reward=-700.17 +/- 45.59\n",
      "Episode length: 741.20 +/- 108.62\n",
      "Eval num_timesteps=800000, episode_reward=-728.02 +/- 173.57\n",
      "Episode length: 884.80 +/- 41.74\n",
      "Eval num_timesteps=805000, episode_reward=-797.42 +/- 39.33\n",
      "Episode length: 909.00 +/- 119.89\n",
      "Eval num_timesteps=810000, episode_reward=-856.62 +/- 27.05\n",
      "Episode length: 1005.60 +/- 76.85\n",
      "Eval num_timesteps=815000, episode_reward=-816.26 +/- 50.51\n",
      "Episode length: 887.60 +/- 50.72\n",
      "Eval num_timesteps=820000, episode_reward=-776.83 +/- 68.24\n",
      "Episode length: 1011.00 +/- 143.27\n",
      "Eval num_timesteps=825000, episode_reward=-888.86 +/- 39.39\n",
      "Episode length: 1413.60 +/- 343.41\n",
      "Eval num_timesteps=830000, episode_reward=-799.63 +/- 25.39\n",
      "Episode length: 853.80 +/- 49.55\n",
      "Eval num_timesteps=835000, episode_reward=-753.90 +/- 113.99\n",
      "Episode length: 874.00 +/- 137.88\n",
      "Eval num_timesteps=840000, episode_reward=-795.84 +/- 93.24\n",
      "Episode length: 1110.80 +/- 246.89\n",
      "Eval num_timesteps=845000, episode_reward=-736.96 +/- 79.45\n",
      "Episode length: 1255.60 +/- 284.42\n",
      "Eval num_timesteps=850000, episode_reward=-785.51 +/- 56.21\n",
      "Episode length: 1343.00 +/- 354.39\n",
      "Eval num_timesteps=855000, episode_reward=-786.89 +/- 145.82\n",
      "Episode length: 1045.00 +/- 182.08\n",
      "Eval num_timesteps=860000, episode_reward=-640.22 +/- 160.54\n",
      "Episode length: 888.80 +/- 135.36\n",
      "Eval num_timesteps=865000, episode_reward=-741.51 +/- 82.17\n",
      "Episode length: 866.00 +/- 48.04\n",
      "Eval num_timesteps=870000, episode_reward=-661.19 +/- 136.55\n",
      "Episode length: 1049.80 +/- 39.76\n",
      "Eval num_timesteps=875000, episode_reward=-900.65 +/- 18.10\n",
      "Episode length: 1460.80 +/- 143.80\n",
      "Eval num_timesteps=880000, episode_reward=-826.21 +/- 71.81\n",
      "Episode length: 1328.80 +/- 185.77\n",
      "Eval num_timesteps=885000, episode_reward=-787.19 +/- 136.88\n",
      "Episode length: 1386.20 +/- 140.80\n",
      "Eval num_timesteps=890000, episode_reward=-822.00 +/- 67.48\n",
      "Episode length: 1296.20 +/- 103.44\n",
      "Eval num_timesteps=895000, episode_reward=-814.68 +/- 115.62\n",
      "Episode length: 2070.20 +/- 123.17\n",
      "Eval num_timesteps=900000, episode_reward=-884.02 +/- 214.52\n",
      "Episode length: 1412.80 +/- 160.79\n",
      "Eval num_timesteps=905000, episode_reward=-635.89 +/- 182.86\n",
      "Episode length: 1481.60 +/- 242.56\n",
      "Eval num_timesteps=910000, episode_reward=-600.41 +/- 112.45\n",
      "Episode length: 1391.20 +/- 231.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=915000, episode_reward=-669.69 +/- 115.38\n",
      "Episode length: 1630.00 +/- 255.05\n",
      "Eval num_timesteps=920000, episode_reward=-728.63 +/- 90.10\n",
      "Episode length: 1924.00 +/- 354.03\n",
      "Eval num_timesteps=925000, episode_reward=-482.58 +/- 176.18\n",
      "Episode length: 2142.20 +/- 528.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=930000, episode_reward=-720.14 +/- 176.71\n",
      "Episode length: 1817.20 +/- 320.96\n",
      "Eval num_timesteps=935000, episode_reward=-746.18 +/- 167.29\n",
      "Episode length: 1709.40 +/- 343.56\n",
      "Eval num_timesteps=940000, episode_reward=-530.36 +/- 263.31\n",
      "Episode length: 2210.00 +/- 585.24\n",
      "Eval num_timesteps=945000, episode_reward=-417.71 +/- 70.12\n",
      "Episode length: 1861.60 +/- 340.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=950000, episode_reward=-483.91 +/- 210.36\n",
      "Episode length: 2268.20 +/- 491.38\n",
      "Eval num_timesteps=955000, episode_reward=-299.99 +/- 145.59\n",
      "Episode length: 2447.00 +/- 539.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=960000, episode_reward=-127.45 +/- 52.23\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=965000, episode_reward=-80.03 +/- 23.10\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=970000, episode_reward=-229.23 +/- 327.66\n",
      "Episode length: 2759.20 +/- 481.60\n",
      "Eval num_timesteps=975000, episode_reward=-228.80 +/- 345.03\n",
      "Episode length: 2986.00 +/- 28.00\n",
      "Eval num_timesteps=980000, episode_reward=-33.53 +/- 38.79\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=985000, episode_reward=-24.23 +/- 28.21\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=990000, episode_reward=-505.58 +/- 405.91\n",
      "Episode length: 2708.60 +/- 396.14\n",
      "Eval num_timesteps=995000, episode_reward=-229.05 +/- 296.46\n",
      "Episode length: 2820.20 +/- 359.60\n",
      "Eval num_timesteps=1000000, episode_reward=-531.08 +/- 273.82\n",
      "Episode length: 2595.80 +/- 341.97\n",
      "Eval num_timesteps=1005000, episode_reward=-395.12 +/- 313.45\n",
      "Episode length: 2709.00 +/- 373.10\n",
      "Eval num_timesteps=1010000, episode_reward=-306.99 +/- 312.99\n",
      "Episode length: 2953.40 +/- 93.20\n",
      "Eval num_timesteps=1015000, episode_reward=-306.81 +/- 196.68\n",
      "Episode length: 2893.60 +/- 138.46\n",
      "Eval num_timesteps=1020000, episode_reward=-564.30 +/- 284.34\n",
      "Episode length: 2083.80 +/- 648.70\n",
      "Eval num_timesteps=1025000, episode_reward=-232.75 +/- 337.30\n",
      "Episode length: 2718.40 +/- 563.20\n",
      "Eval num_timesteps=1030000, episode_reward=-206.75 +/- 141.82\n",
      "Episode length: 2420.20 +/- 711.14\n",
      "Eval num_timesteps=1035000, episode_reward=-140.59 +/- 115.74\n",
      "Episode length: 2651.20 +/- 697.60\n",
      "Eval num_timesteps=1040000, episode_reward=-298.10 +/- 99.11\n",
      "Episode length: 2015.60 +/- 586.89\n",
      "Eval num_timesteps=1045000, episode_reward=-181.63 +/- 88.52\n",
      "Episode length: 2805.80 +/- 388.40\n",
      "Eval num_timesteps=1050000, episode_reward=-274.26 +/- 104.73\n",
      "Episode length: 2021.40 +/- 538.73\n",
      "Eval num_timesteps=1055000, episode_reward=-516.10 +/- 332.83\n",
      "Episode length: 2198.80 +/- 552.40\n",
      "Eval num_timesteps=1060000, episode_reward=-386.73 +/- 29.63\n",
      "Episode length: 2063.00 +/- 376.67\n",
      "Eval num_timesteps=1065000, episode_reward=-146.70 +/- 94.94\n",
      "Episode length: 2792.00 +/- 416.00\n",
      "Eval num_timesteps=1070000, episode_reward=-334.66 +/- 66.39\n",
      "Episode length: 2359.60 +/- 407.30\n",
      "Eval num_timesteps=1075000, episode_reward=-346.73 +/- 130.12\n",
      "Episode length: 2202.60 +/- 412.15\n",
      "Eval num_timesteps=1080000, episode_reward=-123.34 +/- 126.98\n",
      "Episode length: 2822.40 +/- 355.20\n",
      "Eval num_timesteps=1085000, episode_reward=-327.67 +/- 303.33\n",
      "Episode length: 2666.80 +/- 436.74\n",
      "Eval num_timesteps=1090000, episode_reward=-303.03 +/- 92.99\n",
      "Episode length: 2604.60 +/- 243.36\n",
      "Eval num_timesteps=1095000, episode_reward=-293.65 +/- 118.24\n",
      "Episode length: 1942.80 +/- 685.88\n",
      "Eval num_timesteps=1100000, episode_reward=-372.91 +/- 22.20\n",
      "Episode length: 1270.40 +/- 472.72\n",
      "Eval num_timesteps=1105000, episode_reward=-369.79 +/- 151.60\n",
      "Episode length: 1697.00 +/- 834.91\n",
      "Eval num_timesteps=1110000, episode_reward=-408.79 +/- 67.11\n",
      "Episode length: 1540.40 +/- 502.43\n",
      "Eval num_timesteps=1115000, episode_reward=-384.12 +/- 31.32\n",
      "Episode length: 1219.60 +/- 697.96\n",
      "Eval num_timesteps=1120000, episode_reward=-351.41 +/- 4.39\n",
      "Episode length: 1372.00 +/- 562.05\n",
      "Eval num_timesteps=1125000, episode_reward=-271.67 +/- 143.30\n",
      "Episode length: 2727.20 +/- 244.48\n",
      "Eval num_timesteps=1130000, episode_reward=-259.01 +/- 156.26\n",
      "Episode length: 2460.40 +/- 785.51\n",
      "Eval num_timesteps=1135000, episode_reward=-425.14 +/- 132.44\n",
      "Episode length: 2430.00 +/- 339.97\n",
      "Eval num_timesteps=1140000, episode_reward=-347.69 +/- 13.25\n",
      "Episode length: 1681.40 +/- 542.43\n",
      "Eval num_timesteps=1145000, episode_reward=-333.99 +/- 14.14\n",
      "Episode length: 964.60 +/- 237.89\n",
      "Eval num_timesteps=1150000, episode_reward=-352.83 +/- 7.37\n",
      "Episode length: 1378.00 +/- 679.41\n",
      "Eval num_timesteps=1155000, episode_reward=-355.93 +/- 9.53\n",
      "Episode length: 1488.20 +/- 519.32\n",
      "Eval num_timesteps=1160000, episode_reward=-367.54 +/- 3.58\n",
      "Episode length: 1070.00 +/- 321.28\n",
      "Eval num_timesteps=1165000, episode_reward=-347.94 +/- 3.19\n",
      "Episode length: 807.40 +/- 183.14\n",
      "Eval num_timesteps=1170000, episode_reward=-328.20 +/- 8.14\n",
      "Episode length: 1074.20 +/- 231.32\n",
      "Eval num_timesteps=1175000, episode_reward=-266.36 +/- 124.67\n",
      "Episode length: 1559.00 +/- 759.57\n",
      "Eval num_timesteps=1180000, episode_reward=-330.15 +/- 7.78\n",
      "Episode length: 1227.20 +/- 443.68\n",
      "Eval num_timesteps=1185000, episode_reward=-328.53 +/- 3.40\n",
      "Episode length: 909.20 +/- 215.02\n",
      "Eval num_timesteps=1190000, episode_reward=-306.83 +/- 7.57\n",
      "Episode length: 1148.40 +/- 156.28\n",
      "Eval num_timesteps=1195000, episode_reward=-310.52 +/- 5.27\n",
      "Episode length: 1196.20 +/- 353.13\n",
      "Eval num_timesteps=1200000, episode_reward=-315.35 +/- 8.27\n",
      "Episode length: 937.40 +/- 333.15\n",
      "Eval num_timesteps=1205000, episode_reward=-304.34 +/- 6.82\n",
      "Episode length: 842.20 +/- 215.81\n",
      "Eval num_timesteps=1210000, episode_reward=-300.27 +/- 4.77\n",
      "Episode length: 758.60 +/- 150.70\n",
      "Eval num_timesteps=1215000, episode_reward=-304.85 +/- 4.05\n",
      "Episode length: 1212.80 +/- 408.56\n",
      "Eval num_timesteps=1220000, episode_reward=-300.73 +/- 5.70\n",
      "Episode length: 913.60 +/- 370.70\n",
      "Eval num_timesteps=1225000, episode_reward=-289.51 +/- 7.60\n",
      "Episode length: 1005.20 +/- 191.98\n",
      "Eval num_timesteps=1230000, episode_reward=-298.41 +/- 4.27\n",
      "Episode length: 910.20 +/- 367.91\n",
      "Eval num_timesteps=1235000, episode_reward=-309.49 +/- 5.76\n",
      "Episode length: 774.60 +/- 199.96\n",
      "Eval num_timesteps=1240000, episode_reward=-309.59 +/- 9.41\n",
      "Episode length: 824.60 +/- 220.86\n",
      "Eval num_timesteps=1245000, episode_reward=-329.06 +/- 6.59\n",
      "Episode length: 896.40 +/- 78.14\n",
      "Eval num_timesteps=1250000, episode_reward=-326.29 +/- 5.20\n",
      "Episode length: 714.40 +/- 101.62\n",
      "Eval num_timesteps=1255000, episode_reward=-401.22 +/- 64.51\n",
      "Episode length: 1490.00 +/- 789.81\n",
      "Eval num_timesteps=1260000, episode_reward=-336.40 +/- 9.11\n",
      "Episode length: 1125.60 +/- 467.72\n",
      "Eval num_timesteps=1265000, episode_reward=-425.58 +/- 130.84\n",
      "Episode length: 1238.40 +/- 505.03\n",
      "Eval num_timesteps=1270000, episode_reward=-431.42 +/- 8.96\n",
      "Episode length: 854.80 +/- 259.70\n",
      "Eval num_timesteps=1275000, episode_reward=-416.59 +/- 11.34\n",
      "Episode length: 1567.20 +/- 700.54\n",
      "Eval num_timesteps=1280000, episode_reward=-447.80 +/- 7.13\n",
      "Episode length: 1518.40 +/- 510.12\n",
      "Eval num_timesteps=1285000, episode_reward=-329.72 +/- 144.67\n",
      "Episode length: 2355.00 +/- 682.13\n",
      "Eval num_timesteps=1290000, episode_reward=-368.59 +/- 313.03\n",
      "Episode length: 2351.20 +/- 632.97\n",
      "Eval num_timesteps=1295000, episode_reward=-340.53 +/- 170.21\n",
      "Episode length: 1688.40 +/- 822.59\n",
      "Eval num_timesteps=1300000, episode_reward=-184.28 +/- 214.12\n",
      "Episode length: 2423.20 +/- 869.67\n",
      "Eval num_timesteps=1305000, episode_reward=-112.59 +/- 175.76\n",
      "Episode length: 2900.00 +/- 200.00\n",
      "Eval num_timesteps=1310000, episode_reward=-272.66 +/- 196.92\n",
      "Episode length: 2344.60 +/- 620.96\n",
      "Eval num_timesteps=1315000, episode_reward=-508.71 +/- 11.78\n",
      "Episode length: 1842.20 +/- 838.96\n",
      "Eval num_timesteps=1320000, episode_reward=-412.45 +/- 201.28\n",
      "Episode length: 1831.00 +/- 698.14\n",
      "Eval num_timesteps=1325000, episode_reward=-465.38 +/- 15.42\n",
      "Episode length: 1698.40 +/- 566.25\n",
      "Eval num_timesteps=1330000, episode_reward=-308.50 +/- 236.70\n",
      "Episode length: 2233.40 +/- 652.46\n",
      "Eval num_timesteps=1335000, episode_reward=-180.57 +/- 302.07\n",
      "Episode length: 2814.20 +/- 371.60\n",
      "Eval num_timesteps=1340000, episode_reward=-333.04 +/- 241.15\n",
      "Episode length: 1694.40 +/- 1069.02\n",
      "Eval num_timesteps=1345000, episode_reward=-461.04 +/- 57.96\n",
      "Episode length: 1059.40 +/- 171.87\n",
      "Eval num_timesteps=1350000, episode_reward=-497.99 +/- 61.17\n",
      "Episode length: 1175.00 +/- 346.86\n",
      "Eval num_timesteps=1355000, episode_reward=-426.42 +/- 12.86\n",
      "Episode length: 1372.40 +/- 237.24\n",
      "Eval num_timesteps=1360000, episode_reward=-187.92 +/- 168.98\n",
      "Episode length: 2262.00 +/- 923.17\n",
      "Eval num_timesteps=1365000, episode_reward=-226.36 +/- 231.49\n",
      "Episode length: 2662.00 +/- 424.70\n",
      "Eval num_timesteps=1370000, episode_reward=-337.29 +/- 160.02\n",
      "Episode length: 1554.00 +/- 889.59\n",
      "Eval num_timesteps=1375000, episode_reward=-423.11 +/- 51.55\n",
      "Episode length: 936.60 +/- 209.81\n",
      "Eval num_timesteps=1380000, episode_reward=-366.98 +/- 19.93\n",
      "Episode length: 1072.60 +/- 51.16\n",
      "Eval num_timesteps=1385000, episode_reward=-514.49 +/- 97.15\n",
      "Episode length: 1668.00 +/- 591.76\n",
      "Eval num_timesteps=1390000, episode_reward=-445.94 +/- 214.01\n",
      "Episode length: 782.60 +/- 174.73\n",
      "Eval num_timesteps=1395000, episode_reward=-518.01 +/- 156.36\n",
      "Episode length: 579.40 +/- 65.87\n",
      "Eval num_timesteps=1400000, episode_reward=-551.42 +/- 155.41\n",
      "Episode length: 1336.80 +/- 550.76\n",
      "Eval num_timesteps=1405000, episode_reward=-630.62 +/- 245.96\n",
      "Episode length: 938.60 +/- 121.84\n",
      "Eval num_timesteps=1410000, episode_reward=-552.06 +/- 210.36\n",
      "Episode length: 1176.80 +/- 400.27\n",
      "Eval num_timesteps=1415000, episode_reward=-402.40 +/- 154.05\n",
      "Episode length: 684.40 +/- 132.90\n",
      "Eval num_timesteps=1420000, episode_reward=-452.80 +/- 24.56\n",
      "Episode length: 702.60 +/- 42.36\n",
      "Eval num_timesteps=1425000, episode_reward=-499.21 +/- 96.53\n",
      "Episode length: 720.00 +/- 37.04\n",
      "Eval num_timesteps=1430000, episode_reward=-393.48 +/- 16.38\n",
      "Episode length: 598.00 +/- 63.87\n",
      "Eval num_timesteps=1435000, episode_reward=-347.91 +/- 19.76\n",
      "Episode length: 726.20 +/- 55.49\n",
      "Eval num_timesteps=1440000, episode_reward=-326.02 +/- 5.19\n",
      "Episode length: 683.20 +/- 112.91\n",
      "Eval num_timesteps=1445000, episode_reward=-319.83 +/- 13.56\n",
      "Episode length: 776.40 +/- 106.20\n",
      "Eval num_timesteps=1450000, episode_reward=-357.67 +/- 10.30\n",
      "Episode length: 613.40 +/- 123.64\n",
      "Eval num_timesteps=1455000, episode_reward=-375.75 +/- 24.02\n",
      "Episode length: 711.80 +/- 128.60\n",
      "Eval num_timesteps=1460000, episode_reward=-376.17 +/- 8.96\n",
      "Episode length: 588.40 +/- 79.01\n",
      "Eval num_timesteps=1465000, episode_reward=-429.53 +/- 11.47\n",
      "Episode length: 539.60 +/- 96.76\n",
      "Eval num_timesteps=1470000, episode_reward=-474.00 +/- 8.26\n",
      "Episode length: 550.60 +/- 65.47\n",
      "Eval num_timesteps=1475000, episode_reward=-467.50 +/- 8.80\n",
      "Episode length: 537.40 +/- 60.10\n",
      "Eval num_timesteps=1480000, episode_reward=-401.17 +/- 12.99\n",
      "Episode length: 539.60 +/- 88.88\n",
      "Eval num_timesteps=1485000, episode_reward=-396.03 +/- 10.42\n",
      "Episode length: 518.40 +/- 92.25\n",
      "Eval num_timesteps=1490000, episode_reward=-412.27 +/- 5.80\n",
      "Episode length: 490.20 +/- 80.77\n",
      "Eval num_timesteps=1495000, episode_reward=-405.23 +/- 10.35\n",
      "Episode length: 531.80 +/- 23.46\n",
      "Eval num_timesteps=1500000, episode_reward=-405.11 +/- 7.94\n",
      "Episode length: 475.80 +/- 91.12\n",
      "Eval num_timesteps=1505000, episode_reward=-381.64 +/- 12.66\n",
      "Episode length: 483.40 +/- 67.79\n",
      "Eval num_timesteps=1510000, episode_reward=-346.67 +/- 6.46\n",
      "Episode length: 551.40 +/- 67.78\n",
      "Eval num_timesteps=1515000, episode_reward=-322.20 +/- 14.39\n",
      "Episode length: 610.80 +/- 86.93\n",
      "Eval num_timesteps=1520000, episode_reward=-317.25 +/- 6.95\n",
      "Episode length: 575.60 +/- 84.80\n",
      "Eval num_timesteps=1525000, episode_reward=-313.21 +/- 11.42\n",
      "Episode length: 578.80 +/- 131.92\n",
      "Eval num_timesteps=1530000, episode_reward=-300.02 +/- 7.34\n",
      "Episode length: 623.60 +/- 30.45\n",
      "Eval num_timesteps=1535000, episode_reward=-291.03 +/- 5.32\n",
      "Episode length: 674.40 +/- 66.17\n",
      "Eval num_timesteps=1540000, episode_reward=-280.84 +/- 8.41\n",
      "Episode length: 573.00 +/- 58.53\n",
      "Eval num_timesteps=1545000, episode_reward=-288.78 +/- 8.22\n",
      "Episode length: 538.40 +/- 26.84\n",
      "Eval num_timesteps=1550000, episode_reward=-295.93 +/- 6.96\n",
      "Episode length: 549.20 +/- 79.81\n",
      "Eval num_timesteps=1555000, episode_reward=-296.69 +/- 8.91\n",
      "Episode length: 752.20 +/- 89.02\n",
      "Eval num_timesteps=1560000, episode_reward=-295.31 +/- 5.88\n",
      "Episode length: 758.60 +/- 164.39\n",
      "Eval num_timesteps=1565000, episode_reward=-577.71 +/- 128.35\n",
      "Episode length: 757.20 +/- 96.98\n",
      "Eval num_timesteps=1570000, episode_reward=-293.42 +/- 6.54\n",
      "Episode length: 687.00 +/- 77.67\n",
      "Eval num_timesteps=1575000, episode_reward=-326.90 +/- 11.71\n",
      "Episode length: 955.60 +/- 244.74\n",
      "Eval num_timesteps=1580000, episode_reward=-299.24 +/- 9.36\n",
      "Episode length: 877.20 +/- 202.45\n",
      "Eval num_timesteps=1585000, episode_reward=-291.65 +/- 6.57\n",
      "Episode length: 875.80 +/- 125.28\n",
      "Eval num_timesteps=1590000, episode_reward=-294.28 +/- 7.61\n",
      "Episode length: 668.00 +/- 99.54\n",
      "Eval num_timesteps=1595000, episode_reward=-287.36 +/- 8.79\n",
      "Episode length: 644.80 +/- 139.00\n",
      "Eval num_timesteps=1600000, episode_reward=-271.03 +/- 8.32\n",
      "Episode length: 900.00 +/- 257.34\n",
      "Eval num_timesteps=1605000, episode_reward=-271.37 +/- 8.18\n",
      "Episode length: 741.60 +/- 178.22\n",
      "Eval num_timesteps=1610000, episode_reward=-272.73 +/- 7.03\n",
      "Episode length: 815.00 +/- 293.43\n",
      "Eval num_timesteps=1615000, episode_reward=-287.11 +/- 9.45\n",
      "Episode length: 692.20 +/- 120.67\n",
      "Eval num_timesteps=1620000, episode_reward=-602.67 +/- 121.89\n",
      "Episode length: 762.40 +/- 48.66\n",
      "Eval num_timesteps=1625000, episode_reward=-595.48 +/- 81.09\n",
      "Episode length: 886.20 +/- 146.86\n",
      "Eval num_timesteps=1630000, episode_reward=-642.81 +/- 45.16\n",
      "Episode length: 710.00 +/- 120.17\n",
      "Eval num_timesteps=1635000, episode_reward=-401.55 +/- 51.53\n",
      "Episode length: 821.20 +/- 146.10\n",
      "Eval num_timesteps=1640000, episode_reward=-272.77 +/- 4.60\n",
      "Episode length: 769.00 +/- 152.87\n",
      "Eval num_timesteps=1645000, episode_reward=-275.44 +/- 10.97\n",
      "Episode length: 653.40 +/- 86.07\n",
      "Eval num_timesteps=1650000, episode_reward=-271.13 +/- 8.60\n",
      "Episode length: 635.60 +/- 94.43\n",
      "Eval num_timesteps=1655000, episode_reward=-286.89 +/- 8.00\n",
      "Episode length: 541.60 +/- 76.77\n",
      "Eval num_timesteps=1660000, episode_reward=-289.84 +/- 2.87\n",
      "Episode length: 738.80 +/- 92.50\n",
      "Eval num_timesteps=1665000, episode_reward=-288.26 +/- 5.70\n",
      "Episode length: 610.60 +/- 76.79\n",
      "Eval num_timesteps=1670000, episode_reward=-283.35 +/- 3.88\n",
      "Episode length: 575.00 +/- 93.95\n",
      "Eval num_timesteps=1675000, episode_reward=-277.07 +/- 8.10\n",
      "Episode length: 644.60 +/- 81.32\n",
      "Eval num_timesteps=1680000, episode_reward=-354.81 +/- 13.33\n",
      "Episode length: 702.60 +/- 63.02\n",
      "Eval num_timesteps=1685000, episode_reward=-348.08 +/- 28.20\n",
      "Episode length: 671.20 +/- 105.12\n",
      "Eval num_timesteps=1690000, episode_reward=-472.34 +/- 33.59\n",
      "Episode length: 598.00 +/- 46.00\n",
      "Eval num_timesteps=1695000, episode_reward=-347.43 +/- 35.35\n",
      "Episode length: 757.80 +/- 111.26\n",
      "Eval num_timesteps=1700000, episode_reward=-265.38 +/- 8.08\n",
      "Episode length: 682.20 +/- 23.96\n",
      "Eval num_timesteps=1705000, episode_reward=-283.71 +/- 7.91\n",
      "Episode length: 668.60 +/- 92.60\n",
      "Eval num_timesteps=1710000, episode_reward=-284.06 +/- 4.92\n",
      "Episode length: 655.80 +/- 76.19\n",
      "Eval num_timesteps=1715000, episode_reward=-286.29 +/- 6.88\n",
      "Episode length: 598.60 +/- 78.19\n",
      "Eval num_timesteps=1720000, episode_reward=-278.20 +/- 7.85\n",
      "Episode length: 625.40 +/- 65.48\n",
      "Eval num_timesteps=1725000, episode_reward=-274.90 +/- 3.92\n",
      "Episode length: 570.40 +/- 81.87\n",
      "Eval num_timesteps=1730000, episode_reward=-279.92 +/- 10.08\n",
      "Episode length: 583.60 +/- 76.57\n",
      "Eval num_timesteps=1735000, episode_reward=-285.56 +/- 5.32\n",
      "Episode length: 511.80 +/- 20.02\n",
      "Eval num_timesteps=1740000, episode_reward=-298.13 +/- 5.65\n",
      "Episode length: 558.00 +/- 119.00\n",
      "Eval num_timesteps=1745000, episode_reward=-304.38 +/- 7.78\n",
      "Episode length: 542.60 +/- 115.08\n",
      "Eval num_timesteps=1750000, episode_reward=-288.36 +/- 9.74\n",
      "Episode length: 554.80 +/- 135.69\n",
      "Eval num_timesteps=1755000, episode_reward=-293.28 +/- 9.32\n",
      "Episode length: 616.00 +/- 118.61\n",
      "Eval num_timesteps=1760000, episode_reward=-279.94 +/- 9.09\n",
      "Episode length: 523.40 +/- 50.20\n",
      "Eval num_timesteps=1765000, episode_reward=-276.19 +/- 8.55\n",
      "Episode length: 547.40 +/- 130.75\n",
      "Eval num_timesteps=1770000, episode_reward=-269.18 +/- 9.04\n",
      "Episode length: 555.60 +/- 115.95\n",
      "Eval num_timesteps=1775000, episode_reward=-263.34 +/- 8.24\n",
      "Episode length: 468.80 +/- 44.88\n",
      "Eval num_timesteps=1780000, episode_reward=-263.10 +/- 7.80\n",
      "Episode length: 550.00 +/- 50.43\n",
      "Eval num_timesteps=1785000, episode_reward=-261.37 +/- 6.74\n",
      "Episode length: 561.60 +/- 82.24\n",
      "Eval num_timesteps=1790000, episode_reward=-265.50 +/- 11.27\n",
      "Episode length: 576.40 +/- 81.02\n",
      "Eval num_timesteps=1795000, episode_reward=-305.02 +/- 10.56\n",
      "Episode length: 680.80 +/- 48.06\n",
      "Eval num_timesteps=1800000, episode_reward=-377.76 +/- 2.90\n",
      "Episode length: 672.40 +/- 56.27\n",
      "Eval num_timesteps=1805000, episode_reward=-466.45 +/- 48.92\n",
      "Episode length: 716.60 +/- 86.45\n",
      "Eval num_timesteps=1810000, episode_reward=-225.31 +/- 6.42\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=-313.29 +/- 42.34\n",
      "Episode length: 2078.60 +/- 1128.73\n",
      "Eval num_timesteps=1820000, episode_reward=-354.57 +/- 20.42\n",
      "Episode length: 870.60 +/- 116.05\n",
      "Eval num_timesteps=1825000, episode_reward=-346.71 +/- 10.19\n",
      "Episode length: 844.00 +/- 111.22\n",
      "Eval num_timesteps=1830000, episode_reward=-296.27 +/- 27.13\n",
      "Episode length: 755.20 +/- 136.92\n",
      "Eval num_timesteps=1835000, episode_reward=-246.45 +/- 6.93\n",
      "Episode length: 720.20 +/- 80.60\n",
      "Eval num_timesteps=1840000, episode_reward=-338.51 +/- 4.17\n",
      "Episode length: 1088.80 +/- 144.48\n",
      "Eval num_timesteps=1845000, episode_reward=-307.62 +/- 17.70\n",
      "Episode length: 837.60 +/- 160.39\n",
      "Eval num_timesteps=1850000, episode_reward=-265.37 +/- 9.96\n",
      "Episode length: 790.40 +/- 126.74\n",
      "Eval num_timesteps=1855000, episode_reward=-249.11 +/- 7.63\n",
      "Episode length: 597.40 +/- 53.31\n",
      "Eval num_timesteps=1860000, episode_reward=-246.58 +/- 7.31\n",
      "Episode length: 587.20 +/- 127.26\n",
      "Eval num_timesteps=1865000, episode_reward=-254.92 +/- 8.09\n",
      "Episode length: 565.40 +/- 79.96\n",
      "Eval num_timesteps=1870000, episode_reward=-262.31 +/- 6.16\n",
      "Episode length: 581.80 +/- 134.93\n",
      "Eval num_timesteps=1875000, episode_reward=-257.47 +/- 6.20\n",
      "Episode length: 581.40 +/- 113.39\n",
      "Eval num_timesteps=1880000, episode_reward=-251.47 +/- 6.12\n",
      "Episode length: 514.80 +/- 85.74\n",
      "Eval num_timesteps=1885000, episode_reward=-269.10 +/- 7.51\n",
      "Episode length: 484.40 +/- 12.31\n",
      "Eval num_timesteps=1890000, episode_reward=-264.63 +/- 6.50\n",
      "Episode length: 526.60 +/- 106.07\n",
      "Eval num_timesteps=1895000, episode_reward=-263.41 +/- 7.57\n",
      "Episode length: 563.60 +/- 48.53\n",
      "Eval num_timesteps=1900000, episode_reward=-266.92 +/- 5.00\n",
      "Episode length: 548.60 +/- 56.01\n",
      "Eval num_timesteps=1905000, episode_reward=-269.48 +/- 6.96\n",
      "Episode length: 542.00 +/- 73.83\n",
      "Eval num_timesteps=1910000, episode_reward=-306.33 +/- 12.01\n",
      "Episode length: 570.80 +/- 66.13\n",
      "Eval num_timesteps=1915000, episode_reward=-285.05 +/- 4.37\n",
      "Episode length: 505.80 +/- 53.61\n",
      "Eval num_timesteps=1920000, episode_reward=-319.19 +/- 19.91\n",
      "Episode length: 540.80 +/- 48.59\n",
      "Eval num_timesteps=1925000, episode_reward=-310.55 +/- 11.41\n",
      "Episode length: 534.20 +/- 58.88\n",
      "Eval num_timesteps=1930000, episode_reward=-298.79 +/- 8.40\n",
      "Episode length: 543.40 +/- 60.70\n",
      "Eval num_timesteps=1935000, episode_reward=-334.53 +/- 28.79\n",
      "Episode length: 616.80 +/- 75.98\n",
      "Eval num_timesteps=1940000, episode_reward=-288.88 +/- 6.14\n",
      "Episode length: 532.20 +/- 47.05\n",
      "Eval num_timesteps=1945000, episode_reward=-317.84 +/- 9.12\n",
      "Episode length: 649.80 +/- 30.33\n",
      "Eval num_timesteps=1950000, episode_reward=-279.62 +/- 7.35\n",
      "Episode length: 563.60 +/- 45.19\n",
      "Eval num_timesteps=1955000, episode_reward=-271.65 +/- 7.14\n",
      "Episode length: 499.80 +/- 55.73\n",
      "Eval num_timesteps=1960000, episode_reward=-259.97 +/- 7.17\n",
      "Episode length: 527.20 +/- 41.87\n",
      "Eval num_timesteps=1965000, episode_reward=-264.22 +/- 8.35\n",
      "Episode length: 595.20 +/- 72.32\n",
      "Eval num_timesteps=1970000, episode_reward=-287.15 +/- 8.75\n",
      "Episode length: 657.20 +/- 73.71\n",
      "Eval num_timesteps=1975000, episode_reward=-265.22 +/- 10.28\n",
      "Episode length: 585.80 +/- 68.86\n",
      "Eval num_timesteps=1980000, episode_reward=-265.94 +/- 7.84\n",
      "Episode length: 606.80 +/- 69.10\n",
      "Eval num_timesteps=1985000, episode_reward=-330.11 +/- 4.63\n",
      "Episode length: 747.60 +/- 67.95\n",
      "Eval num_timesteps=1990000, episode_reward=-289.27 +/- 7.28\n",
      "Episode length: 635.40 +/- 45.83\n",
      "Eval num_timesteps=1995000, episode_reward=-379.29 +/- 99.97\n",
      "Episode length: 2469.00 +/- 787.62\n",
      "Eval num_timesteps=2000000, episode_reward=-280.74 +/- 18.76\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2005000, episode_reward=-357.66 +/- 11.27\n",
      "Episode length: 798.00 +/- 80.09\n",
      "Eval num_timesteps=2010000, episode_reward=-442.45 +/- 74.03\n",
      "Episode length: 2363.60 +/- 839.85\n",
      "Eval num_timesteps=2015000, episode_reward=-378.81 +/- 16.16\n",
      "Episode length: 798.40 +/- 131.58\n",
      "Eval num_timesteps=2020000, episode_reward=-292.77 +/- 57.26\n",
      "Episode length: 1598.60 +/- 1144.40\n",
      "Eval num_timesteps=2025000, episode_reward=-184.82 +/- 4.03\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2030000, episode_reward=-296.40 +/- 92.57\n",
      "Episode length: 2653.60 +/- 692.80\n",
      "Eval num_timesteps=2035000, episode_reward=-189.22 +/- 14.90\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=-248.45 +/- 61.60\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2045000, episode_reward=-307.73 +/- 17.00\n",
      "Episode length: 968.00 +/- 143.55\n",
      "Eval num_timesteps=2050000, episode_reward=-387.17 +/- 76.88\n",
      "Episode length: 1077.20 +/- 188.00\n",
      "Eval num_timesteps=2055000, episode_reward=-285.68 +/- 9.10\n",
      "Episode length: 764.20 +/- 94.72\n",
      "Eval num_timesteps=2060000, episode_reward=-283.44 +/- 7.31\n",
      "Episode length: 665.00 +/- 113.31\n",
      "Eval num_timesteps=2065000, episode_reward=-278.85 +/- 9.65\n",
      "Episode length: 836.60 +/- 30.88\n",
      "Eval num_timesteps=2070000, episode_reward=-293.47 +/- 6.86\n",
      "Episode length: 729.80 +/- 84.22\n",
      "Eval num_timesteps=2075000, episode_reward=-313.32 +/- 12.23\n",
      "Episode length: 898.40 +/- 50.78\n",
      "Eval num_timesteps=2080000, episode_reward=-498.26 +/- 178.65\n",
      "Episode length: 911.40 +/- 113.79\n",
      "Eval num_timesteps=2085000, episode_reward=-292.80 +/- 1.71\n",
      "Episode length: 705.40 +/- 82.65\n",
      "Eval num_timesteps=2090000, episode_reward=-296.21 +/- 6.41\n",
      "Episode length: 840.80 +/- 27.68\n",
      "Eval num_timesteps=2095000, episode_reward=-289.18 +/- 5.49\n",
      "Episode length: 794.20 +/- 300.84\n",
      "Eval num_timesteps=2100000, episode_reward=-377.21 +/- 25.21\n",
      "Episode length: 1048.80 +/- 240.90\n",
      "Eval num_timesteps=2105000, episode_reward=-462.79 +/- 65.48\n",
      "Episode length: 973.40 +/- 130.06\n",
      "Eval num_timesteps=2110000, episode_reward=-456.64 +/- 134.43\n",
      "Episode length: 2993.00 +/- 14.00\n",
      "Eval num_timesteps=2115000, episode_reward=-245.58 +/- 134.95\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=-302.32 +/- 5.22\n",
      "Episode length: 1054.20 +/- 95.01\n",
      "Eval num_timesteps=2125000, episode_reward=-296.62 +/- 8.01\n",
      "Episode length: 860.00 +/- 132.25\n",
      "Eval num_timesteps=2130000, episode_reward=-303.42 +/- 9.78\n",
      "Episode length: 1142.80 +/- 149.46\n",
      "Eval num_timesteps=2135000, episode_reward=-313.57 +/- 4.89\n",
      "Episode length: 1078.40 +/- 210.52\n",
      "Eval num_timesteps=2140000, episode_reward=-306.87 +/- 7.81\n",
      "Episode length: 870.80 +/- 92.13\n",
      "Eval num_timesteps=2145000, episode_reward=-318.56 +/- 10.24\n",
      "Episode length: 756.80 +/- 122.37\n",
      "Eval num_timesteps=2150000, episode_reward=-307.63 +/- 8.95\n",
      "Episode length: 720.60 +/- 109.40\n",
      "Eval num_timesteps=2155000, episode_reward=-307.56 +/- 7.03\n",
      "Episode length: 824.40 +/- 70.79\n",
      "Eval num_timesteps=2160000, episode_reward=-311.03 +/- 5.37\n",
      "Episode length: 867.40 +/- 78.40\n",
      "Eval num_timesteps=2165000, episode_reward=-305.73 +/- 8.56\n",
      "Episode length: 969.00 +/- 63.54\n",
      "Eval num_timesteps=2170000, episode_reward=-309.03 +/- 9.40\n",
      "Episode length: 717.60 +/- 88.13\n",
      "Eval num_timesteps=2175000, episode_reward=-325.05 +/- 8.43\n",
      "Episode length: 711.00 +/- 86.79\n",
      "Eval num_timesteps=2180000, episode_reward=-309.96 +/- 9.57\n",
      "Episode length: 763.20 +/- 91.69\n",
      "Eval num_timesteps=2185000, episode_reward=-313.43 +/- 6.95\n",
      "Episode length: 828.60 +/- 17.78\n",
      "Eval num_timesteps=2190000, episode_reward=-311.41 +/- 7.55\n",
      "Episode length: 676.00 +/- 97.60\n",
      "Eval num_timesteps=2195000, episode_reward=-303.73 +/- 6.44\n",
      "Episode length: 678.00 +/- 66.15\n",
      "Eval num_timesteps=2200000, episode_reward=-304.51 +/- 5.73\n",
      "Episode length: 765.40 +/- 67.97\n",
      "Eval num_timesteps=2205000, episode_reward=-303.96 +/- 4.96\n",
      "Episode length: 684.80 +/- 93.94\n",
      "Eval num_timesteps=2210000, episode_reward=-310.54 +/- 11.51\n",
      "Episode length: 642.80 +/- 83.66\n",
      "Eval num_timesteps=2215000, episode_reward=-317.47 +/- 5.65\n",
      "Episode length: 539.60 +/- 31.78\n",
      "Eval num_timesteps=2220000, episode_reward=-324.29 +/- 5.56\n",
      "Episode length: 648.20 +/- 82.60\n",
      "Eval num_timesteps=2225000, episode_reward=-312.69 +/- 3.97\n",
      "Episode length: 659.40 +/- 52.94\n",
      "Eval num_timesteps=2230000, episode_reward=-318.18 +/- 8.55\n",
      "Episode length: 610.40 +/- 82.04\n",
      "Eval num_timesteps=2235000, episode_reward=-314.37 +/- 8.71\n",
      "Episode length: 572.00 +/- 35.37\n",
      "Eval num_timesteps=2240000, episode_reward=-314.85 +/- 6.40\n",
      "Episode length: 656.40 +/- 107.00\n",
      "Eval num_timesteps=2245000, episode_reward=-409.64 +/- 89.80\n",
      "Episode length: 890.40 +/- 134.77\n",
      "Eval num_timesteps=2250000, episode_reward=-399.90 +/- 119.54\n",
      "Episode length: 1567.60 +/- 766.81\n",
      "Eval num_timesteps=2255000, episode_reward=-352.11 +/- 14.34\n",
      "Episode length: 1101.00 +/- 176.78\n",
      "Eval num_timesteps=2260000, episode_reward=-309.48 +/- 7.87\n",
      "Episode length: 852.80 +/- 109.90\n",
      "Eval num_timesteps=2265000, episode_reward=-321.19 +/- 8.39\n",
      "Episode length: 945.20 +/- 85.00\n",
      "Eval num_timesteps=2270000, episode_reward=-311.19 +/- 8.21\n",
      "Episode length: 922.20 +/- 160.92\n",
      "Eval num_timesteps=2275000, episode_reward=-307.86 +/- 6.02\n",
      "Episode length: 887.20 +/- 95.10\n",
      "Eval num_timesteps=2280000, episode_reward=-361.76 +/- 17.96\n",
      "Episode length: 1077.60 +/- 53.11\n",
      "Eval num_timesteps=2285000, episode_reward=-312.94 +/- 7.83\n",
      "Episode length: 834.60 +/- 107.90\n",
      "Eval num_timesteps=2290000, episode_reward=-336.87 +/- 9.31\n",
      "Episode length: 1065.00 +/- 51.27\n",
      "Eval num_timesteps=2295000, episode_reward=-350.55 +/- 91.27\n",
      "Episode length: 1890.40 +/- 907.07\n",
      "Eval num_timesteps=2300000, episode_reward=-336.10 +/- 13.31\n",
      "Episode length: 1194.80 +/- 89.97\n",
      "Eval num_timesteps=2305000, episode_reward=-321.50 +/- 5.19\n",
      "Episode length: 1302.40 +/- 91.39\n",
      "Eval num_timesteps=2310000, episode_reward=-343.10 +/- 8.71\n",
      "Episode length: 871.40 +/- 60.41\n",
      "Eval num_timesteps=2315000, episode_reward=-359.58 +/- 8.23\n",
      "Episode length: 750.80 +/- 60.44\n",
      "Eval num_timesteps=2320000, episode_reward=-356.23 +/- 3.33\n",
      "Episode length: 668.00 +/- 103.54\n",
      "Eval num_timesteps=2325000, episode_reward=-368.75 +/- 7.10\n",
      "Episode length: 745.20 +/- 56.19\n",
      "Eval num_timesteps=2330000, episode_reward=-403.94 +/- 6.20\n",
      "Episode length: 726.20 +/- 30.03\n",
      "Eval num_timesteps=2335000, episode_reward=-398.20 +/- 7.94\n",
      "Episode length: 668.00 +/- 83.17\n",
      "Eval num_timesteps=2340000, episode_reward=-398.34 +/- 5.26\n",
      "Episode length: 753.20 +/- 119.24\n",
      "Eval num_timesteps=2345000, episode_reward=-382.35 +/- 14.25\n",
      "Episode length: 883.40 +/- 75.64\n",
      "Eval num_timesteps=2350000, episode_reward=-393.07 +/- 5.98\n",
      "Episode length: 768.20 +/- 70.99\n",
      "Eval num_timesteps=2355000, episode_reward=-369.70 +/- 7.63\n",
      "Episode length: 653.40 +/- 94.42\n",
      "Eval num_timesteps=2360000, episode_reward=-220.65 +/- 177.93\n",
      "Episode length: 2017.00 +/- 997.15\n",
      "Eval num_timesteps=2365000, episode_reward=-364.74 +/- 6.10\n",
      "Episode length: 1264.20 +/- 391.28\n",
      "Eval num_timesteps=2370000, episode_reward=-354.91 +/- 2.57\n",
      "Episode length: 1943.20 +/- 117.21\n",
      "Eval num_timesteps=2375000, episode_reward=-371.39 +/- 7.38\n",
      "Episode length: 1901.40 +/- 125.82\n",
      "Eval num_timesteps=2380000, episode_reward=-372.03 +/- 3.84\n",
      "Episode length: 820.80 +/- 214.86\n",
      "Eval num_timesteps=2385000, episode_reward=-361.01 +/- 8.52\n",
      "Episode length: 853.40 +/- 138.12\n",
      "Eval num_timesteps=2390000, episode_reward=-191.10 +/- 158.78\n",
      "Episode length: 2104.60 +/- 1096.65\n",
      "Eval num_timesteps=2395000, episode_reward=-235.68 +/- 181.31\n",
      "Episode length: 2350.80 +/- 762.85\n",
      "Eval num_timesteps=2400000, episode_reward=-359.51 +/- 9.37\n",
      "Episode length: 1413.00 +/- 682.43\n",
      "Eval num_timesteps=2405000, episode_reward=-342.31 +/- 6.24\n",
      "Episode length: 829.00 +/- 309.02\n",
      "Eval num_timesteps=2410000, episode_reward=-347.54 +/- 15.58\n",
      "Episode length: 1559.40 +/- 775.16\n",
      "Eval num_timesteps=2415000, episode_reward=-272.03 +/- 113.02\n",
      "Episode length: 1835.60 +/- 871.11\n",
      "Eval num_timesteps=2420000, episode_reward=-342.28 +/- 18.71\n",
      "Episode length: 1272.20 +/- 620.42\n",
      "Eval num_timesteps=2425000, episode_reward=-378.14 +/- 7.68\n",
      "Episode length: 1033.60 +/- 249.25\n",
      "Eval num_timesteps=2430000, episode_reward=-385.21 +/- 2.88\n",
      "Episode length: 835.40 +/- 189.37\n",
      "Eval num_timesteps=2435000, episode_reward=-377.05 +/- 6.87\n",
      "Episode length: 961.60 +/- 179.00\n",
      "Eval num_timesteps=2440000, episode_reward=-362.63 +/- 6.69\n",
      "Episode length: 890.60 +/- 140.23\n",
      "Eval num_timesteps=2445000, episode_reward=-362.48 +/- 10.75\n",
      "Episode length: 1005.80 +/- 242.53\n",
      "Eval num_timesteps=2450000, episode_reward=-352.62 +/- 4.61\n",
      "Episode length: 1110.40 +/- 245.76\n",
      "Eval num_timesteps=2455000, episode_reward=-335.78 +/- 9.55\n",
      "Episode length: 847.80 +/- 140.21\n",
      "Eval num_timesteps=2460000, episode_reward=-340.46 +/- 6.55\n",
      "Episode length: 842.00 +/- 123.52\n",
      "Eval num_timesteps=2465000, episode_reward=-338.65 +/- 1.36\n",
      "Episode length: 985.80 +/- 159.95\n",
      "Eval num_timesteps=2470000, episode_reward=-325.32 +/- 4.39\n",
      "Episode length: 942.20 +/- 115.37\n",
      "Eval num_timesteps=2475000, episode_reward=-330.80 +/- 6.03\n",
      "Episode length: 913.80 +/- 85.99\n",
      "Eval num_timesteps=2480000, episode_reward=-332.34 +/- 7.89\n",
      "Episode length: 780.80 +/- 92.26\n",
      "Eval num_timesteps=2485000, episode_reward=-334.49 +/- 4.72\n",
      "Episode length: 941.20 +/- 95.11\n",
      "Eval num_timesteps=2490000, episode_reward=-317.40 +/- 8.56\n",
      "Episode length: 874.80 +/- 98.77\n",
      "Eval num_timesteps=2495000, episode_reward=-312.23 +/- 4.33\n",
      "Episode length: 917.60 +/- 111.15\n",
      "Eval num_timesteps=2500000, episode_reward=-323.80 +/- 4.96\n",
      "Episode length: 858.00 +/- 78.88\n",
      "Eval num_timesteps=2505000, episode_reward=-307.77 +/- 2.39\n",
      "Episode length: 841.00 +/- 116.74\n",
      "Eval num_timesteps=2510000, episode_reward=-311.31 +/- 5.46\n",
      "Episode length: 764.60 +/- 102.64\n",
      "Eval num_timesteps=2515000, episode_reward=-325.60 +/- 8.52\n",
      "Episode length: 575.00 +/- 55.70\n",
      "Eval num_timesteps=2520000, episode_reward=-319.35 +/- 7.11\n",
      "Episode length: 647.80 +/- 79.41\n",
      "Eval num_timesteps=2525000, episode_reward=-321.93 +/- 9.49\n",
      "Episode length: 594.20 +/- 55.15\n",
      "Eval num_timesteps=2530000, episode_reward=-310.29 +/- 2.96\n",
      "Episode length: 625.00 +/- 42.06\n",
      "Eval num_timesteps=2535000, episode_reward=-314.54 +/- 8.85\n",
      "Episode length: 605.00 +/- 82.85\n",
      "Eval num_timesteps=2540000, episode_reward=-308.92 +/- 5.20\n",
      "Episode length: 694.80 +/- 77.46\n",
      "Eval num_timesteps=2545000, episode_reward=-304.87 +/- 4.96\n",
      "Episode length: 563.80 +/- 80.36\n",
      "Eval num_timesteps=2550000, episode_reward=-290.24 +/- 8.67\n",
      "Episode length: 634.80 +/- 78.05\n",
      "Eval num_timesteps=2555000, episode_reward=-379.95 +/- 19.74\n",
      "Episode length: 789.80 +/- 103.55\n",
      "Eval num_timesteps=2560000, episode_reward=-314.67 +/- 9.58\n",
      "Episode length: 719.80 +/- 92.77\n",
      "Eval num_timesteps=2565000, episode_reward=-299.87 +/- 3.63\n",
      "Episode length: 626.60 +/- 38.34\n",
      "Eval num_timesteps=2570000, episode_reward=-299.16 +/- 5.98\n",
      "Episode length: 672.00 +/- 65.67\n",
      "Eval num_timesteps=2575000, episode_reward=-300.43 +/- 8.57\n",
      "Episode length: 747.00 +/- 67.72\n",
      "Eval num_timesteps=2580000, episode_reward=-294.08 +/- 7.00\n",
      "Episode length: 626.40 +/- 72.82\n",
      "Eval num_timesteps=2585000, episode_reward=-315.16 +/- 5.98\n",
      "Episode length: 762.80 +/- 64.22\n",
      "Eval num_timesteps=2590000, episode_reward=-292.09 +/- 6.12\n",
      "Episode length: 729.20 +/- 51.69\n",
      "Eval num_timesteps=2595000, episode_reward=-284.49 +/- 5.02\n",
      "Episode length: 720.00 +/- 65.10\n",
      "Eval num_timesteps=2600000, episode_reward=-285.96 +/- 5.46\n",
      "Episode length: 806.60 +/- 57.21\n",
      "Eval num_timesteps=2605000, episode_reward=-264.59 +/- 7.45\n",
      "Episode length: 634.60 +/- 84.58\n",
      "Eval num_timesteps=2610000, episode_reward=-281.61 +/- 8.24\n",
      "Episode length: 792.80 +/- 78.48\n",
      "Eval num_timesteps=2615000, episode_reward=-606.18 +/- 87.10\n",
      "Episode length: 2094.60 +/- 726.76\n",
      "Eval num_timesteps=2620000, episode_reward=-303.84 +/- 8.27\n",
      "Episode length: 1203.40 +/- 255.83\n",
      "Eval num_timesteps=2625000, episode_reward=-102.46 +/- 1.35\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2630000, episode_reward=-299.11 +/- 9.09\n",
      "Episode length: 1434.40 +/- 215.47\n",
      "Eval num_timesteps=2635000, episode_reward=-270.58 +/- 6.83\n",
      "Episode length: 1301.60 +/- 293.31\n",
      "Eval num_timesteps=2640000, episode_reward=-260.32 +/- 10.75\n",
      "Episode length: 1105.40 +/- 330.20\n",
      "Eval num_timesteps=2645000, episode_reward=-270.41 +/- 7.97\n",
      "Episode length: 1130.40 +/- 218.87\n",
      "Eval num_timesteps=2650000, episode_reward=-268.41 +/- 6.75\n",
      "Episode length: 1027.80 +/- 264.79\n",
      "Eval num_timesteps=2655000, episode_reward=-270.72 +/- 8.23\n",
      "Episode length: 579.00 +/- 22.62\n",
      "Eval num_timesteps=2660000, episode_reward=-282.38 +/- 7.86\n",
      "Episode length: 711.80 +/- 58.55\n",
      "Eval num_timesteps=2665000, episode_reward=-269.14 +/- 10.36\n",
      "Episode length: 792.00 +/- 92.59\n",
      "Eval num_timesteps=2670000, episode_reward=-263.93 +/- 6.84\n",
      "Episode length: 771.40 +/- 121.32\n",
      "Eval num_timesteps=2675000, episode_reward=-258.20 +/- 5.42\n",
      "Episode length: 1017.80 +/- 157.84\n",
      "Eval num_timesteps=2680000, episode_reward=-266.27 +/- 8.61\n",
      "Episode length: 947.60 +/- 181.46\n",
      "Eval num_timesteps=2685000, episode_reward=-362.62 +/- 55.08\n",
      "Episode length: 1790.00 +/- 660.65\n",
      "Eval num_timesteps=2690000, episode_reward=-295.09 +/- 6.05\n",
      "Episode length: 927.00 +/- 132.49\n",
      "Eval num_timesteps=2695000, episode_reward=-239.55 +/- 123.04\n",
      "Episode length: 2941.00 +/- 118.00\n",
      "Eval num_timesteps=2700000, episode_reward=-74.25 +/- 64.06\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2705000, episode_reward=-239.00 +/- 101.28\n",
      "Episode length: 1274.00 +/- 870.60\n",
      "Eval num_timesteps=2710000, episode_reward=-195.57 +/- 107.08\n",
      "Episode length: 1718.40 +/- 1050.49\n",
      "Eval num_timesteps=2715000, episode_reward=-138.52 +/- 74.70\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2720000, episode_reward=-105.27 +/- 109.88\n",
      "Episode length: 2597.80 +/- 804.40\n",
      "Eval num_timesteps=2725000, episode_reward=-233.51 +/- 99.84\n",
      "Episode length: 1300.00 +/- 859.84\n",
      "Eval num_timesteps=2730000, episode_reward=-179.40 +/- 124.82\n",
      "Episode length: 1833.00 +/- 957.86\n",
      "Eval num_timesteps=2735000, episode_reward=-185.71 +/- 124.69\n",
      "Episode length: 1820.80 +/- 965.56\n",
      "Eval num_timesteps=2740000, episode_reward=-147.23 +/- 13.39\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2745000, episode_reward=-21.88 +/- 5.42\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2750000, episode_reward=-170.82 +/- 109.79\n",
      "Episode length: 2623.80 +/- 752.40\n",
      "Eval num_timesteps=2755000, episode_reward=-309.59 +/- 17.93\n",
      "Episode length: 1779.40 +/- 883.19\n",
      "Eval num_timesteps=2760000, episode_reward=-298.52 +/- 11.15\n",
      "Episode length: 990.80 +/- 113.52\n",
      "Eval num_timesteps=2765000, episode_reward=-252.56 +/- 92.75\n",
      "Episode length: 1263.20 +/- 872.00\n",
      "Eval num_timesteps=2770000, episode_reward=-256.71 +/- 179.65\n",
      "Episode length: 2910.40 +/- 110.70\n",
      "Eval num_timesteps=2775000, episode_reward=-138.69 +/- 4.95\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2780000, episode_reward=-273.52 +/- 70.37\n",
      "Episode length: 1643.80 +/- 1088.78\n",
      "Eval num_timesteps=2785000, episode_reward=-436.50 +/- 126.22\n",
      "Episode length: 1536.20 +/- 290.19\n",
      "Eval num_timesteps=2790000, episode_reward=-265.22 +/- 8.16\n",
      "Episode length: 777.20 +/- 119.29\n",
      "Eval num_timesteps=2795000, episode_reward=-259.54 +/- 4.93\n",
      "Episode length: 678.60 +/- 99.30\n",
      "Eval num_timesteps=2800000, episode_reward=-281.85 +/- 26.95\n",
      "Episode length: 1014.60 +/- 462.63\n",
      "Eval num_timesteps=2805000, episode_reward=-288.38 +/- 32.37\n",
      "Episode length: 1053.00 +/- 728.32\n",
      "Eval num_timesteps=2810000, episode_reward=-354.01 +/- 31.06\n",
      "Episode length: 905.80 +/- 342.08\n",
      "Eval num_timesteps=2815000, episode_reward=-405.04 +/- 91.16\n",
      "Episode length: 1240.00 +/- 586.59\n",
      "Eval num_timesteps=2820000, episode_reward=-385.98 +/- 90.90\n",
      "Episode length: 881.80 +/- 410.58\n",
      "Eval num_timesteps=2825000, episode_reward=-308.21 +/- 16.47\n",
      "Episode length: 1127.20 +/- 942.10\n",
      "Eval num_timesteps=2830000, episode_reward=-361.15 +/- 78.53\n",
      "Episode length: 1241.20 +/- 572.00\n",
      "Eval num_timesteps=2835000, episode_reward=-324.94 +/- 73.30\n",
      "Episode length: 1045.00 +/- 488.08\n",
      "Eval num_timesteps=2840000, episode_reward=-385.51 +/- 150.42\n",
      "Episode length: 811.00 +/- 139.45\n",
      "Eval num_timesteps=2845000, episode_reward=-289.21 +/- 18.90\n",
      "Episode length: 928.00 +/- 166.31\n",
      "Eval num_timesteps=2850000, episode_reward=-354.07 +/- 89.05\n",
      "Episode length: 1758.80 +/- 929.74\n",
      "Eval num_timesteps=2855000, episode_reward=-231.68 +/- 18.78\n",
      "Episode length: 1640.40 +/- 1110.41\n",
      "Eval num_timesteps=2860000, episode_reward=-387.08 +/- 145.08\n",
      "Episode length: 2253.40 +/- 690.72\n",
      "Eval num_timesteps=2865000, episode_reward=-289.92 +/- 6.40\n",
      "Episode length: 939.40 +/- 179.27\n",
      "Eval num_timesteps=2870000, episode_reward=-289.20 +/- 26.79\n",
      "Episode length: 1072.00 +/- 351.15\n",
      "Eval num_timesteps=2875000, episode_reward=-271.86 +/- 11.50\n",
      "Episode length: 1040.80 +/- 320.11\n",
      "Eval num_timesteps=2880000, episode_reward=-213.64 +/- 84.39\n",
      "Episode length: 1828.20 +/- 741.41\n",
      "Eval num_timesteps=2885000, episode_reward=-503.44 +/- 251.76\n",
      "Episode length: 1692.40 +/- 895.10\n",
      "Eval num_timesteps=2890000, episode_reward=-743.89 +/- 16.04\n",
      "Episode length: 568.60 +/- 63.91\n",
      "Eval num_timesteps=2895000, episode_reward=-376.00 +/- 177.54\n",
      "Episode length: 1362.60 +/- 675.43\n",
      "Eval num_timesteps=2900000, episode_reward=-288.88 +/- 42.50\n",
      "Episode length: 1543.00 +/- 445.27\n",
      "Eval num_timesteps=2905000, episode_reward=-138.30 +/- 74.44\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2910000, episode_reward=-89.10 +/- 37.95\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2915000, episode_reward=-201.91 +/- 104.18\n",
      "Episode length: 2070.40 +/- 778.40\n",
      "Eval num_timesteps=2920000, episode_reward=-224.32 +/- 76.66\n",
      "Episode length: 1970.20 +/- 853.89\n",
      "Eval num_timesteps=2925000, episode_reward=-60.16 +/- 10.45\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2930000, episode_reward=-79.22 +/- 20.18\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2935000, episode_reward=-199.10 +/- 118.61\n",
      "Episode length: 2230.40 +/- 776.94\n",
      "Eval num_timesteps=2940000, episode_reward=-166.51 +/- 135.26\n",
      "Episode length: 1807.40 +/- 978.65\n",
      "Eval num_timesteps=2945000, episode_reward=-23.27 +/- 26.29\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2950000, episode_reward=-31.50 +/- 21.61\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2955000, episode_reward=-114.86 +/- 62.80\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=2960000, episode_reward=-114.98 +/- 83.78\n",
      "Episode length: 2880.00 +/- 240.00\n",
      "Eval num_timesteps=2965000, episode_reward=-307.02 +/- 13.16\n",
      "Episode length: 1734.40 +/- 396.19\n",
      "Eval num_timesteps=2970000, episode_reward=-326.04 +/- 36.27\n",
      "Episode length: 1797.00 +/- 738.56\n",
      "Eval num_timesteps=2975000, episode_reward=-335.63 +/- 52.47\n",
      "Episode length: 1890.40 +/- 728.99\n",
      "Eval num_timesteps=2980000, episode_reward=-374.28 +/- 93.77\n",
      "Episode length: 1233.60 +/- 469.36\n",
      "Eval num_timesteps=2985000, episode_reward=-380.48 +/- 171.14\n",
      "Episode length: 1836.00 +/- 615.20\n",
      "Eval num_timesteps=2990000, episode_reward=-72.21 +/- 131.93\n",
      "Episode length: 2751.20 +/- 497.60\n",
      "Eval num_timesteps=2995000, episode_reward=-416.43 +/- 65.01\n",
      "Episode length: 1490.00 +/- 188.15\n",
      "Eval num_timesteps=3000000, episode_reward=-283.74 +/- 151.85\n",
      "Episode length: 2826.20 +/- 174.55\n",
      "Eval num_timesteps=3005000, episode_reward=-397.83 +/- 29.25\n",
      "Episode length: 1077.40 +/- 248.78\n",
      "Eval num_timesteps=3010000, episode_reward=-157.12 +/- 90.20\n",
      "Episode length: 2622.80 +/- 754.40\n",
      "Eval num_timesteps=3015000, episode_reward=-336.12 +/- 162.62\n",
      "Episode length: 1794.80 +/- 667.93\n",
      "Eval num_timesteps=3020000, episode_reward=-381.68 +/- 28.91\n",
      "Episode length: 1165.40 +/- 300.66\n",
      "Eval num_timesteps=3025000, episode_reward=-243.51 +/- 164.57\n",
      "Episode length: 2822.00 +/- 202.11\n",
      "Eval num_timesteps=3030000, episode_reward=-417.51 +/- 61.96\n",
      "Episode length: 1787.40 +/- 806.10\n",
      "Eval num_timesteps=3035000, episode_reward=-262.45 +/- 212.17\n",
      "Episode length: 2178.20 +/- 762.75\n",
      "Eval num_timesteps=3040000, episode_reward=-515.73 +/- 84.13\n",
      "Episode length: 978.40 +/- 190.89\n",
      "Eval num_timesteps=3045000, episode_reward=-513.20 +/- 54.98\n",
      "Episode length: 1189.00 +/- 214.34\n",
      "Eval num_timesteps=3050000, episode_reward=-423.60 +/- 20.21\n",
      "Episode length: 1143.20 +/- 109.79\n",
      "Eval num_timesteps=3055000, episode_reward=-312.69 +/- 144.29\n",
      "Episode length: 2187.40 +/- 899.09\n",
      "Eval num_timesteps=3060000, episode_reward=-393.83 +/- 20.48\n",
      "Episode length: 1550.80 +/- 441.32\n",
      "Eval num_timesteps=3065000, episode_reward=-434.19 +/- 15.38\n",
      "Episode length: 945.60 +/- 189.20\n",
      "Eval num_timesteps=3070000, episode_reward=-517.63 +/- 56.60\n",
      "Episode length: 948.60 +/- 154.93\n",
      "Eval num_timesteps=3075000, episode_reward=-579.96 +/- 73.58\n",
      "Episode length: 893.40 +/- 165.64\n",
      "Eval num_timesteps=3080000, episode_reward=-403.91 +/- 25.40\n",
      "Episode length: 1088.80 +/- 275.40\n",
      "Eval num_timesteps=3085000, episode_reward=-342.97 +/- 46.95\n",
      "Episode length: 2189.60 +/- 651.90\n",
      "Eval num_timesteps=3090000, episode_reward=-348.71 +/- 60.16\n",
      "Episode length: 1678.60 +/- 750.36\n",
      "Eval num_timesteps=3095000, episode_reward=-309.57 +/- 53.07\n",
      "Episode length: 1207.80 +/- 296.47\n",
      "Eval num_timesteps=3100000, episode_reward=-303.17 +/- 8.07\n",
      "Episode length: 1226.40 +/- 251.49\n",
      "Eval num_timesteps=3105000, episode_reward=-267.69 +/- 228.81\n",
      "Episode length: 1997.80 +/- 876.79\n",
      "Eval num_timesteps=3110000, episode_reward=-312.10 +/- 162.25\n",
      "Episode length: 1744.60 +/- 820.25\n",
      "Eval num_timesteps=3115000, episode_reward=-232.09 +/- 190.20\n",
      "Episode length: 2137.20 +/- 827.62\n",
      "Eval num_timesteps=3120000, episode_reward=-201.65 +/- 164.63\n",
      "Episode length: 1787.00 +/- 992.67\n",
      "Eval num_timesteps=3125000, episode_reward=-205.45 +/- 167.69\n",
      "Episode length: 1816.80 +/- 976.50\n",
      "Eval num_timesteps=3130000, episode_reward=-343.97 +/- 24.83\n",
      "Episode length: 1259.80 +/- 465.45\n",
      "Eval num_timesteps=3135000, episode_reward=-333.30 +/- 20.79\n",
      "Episode length: 967.40 +/- 160.78\n",
      "Eval num_timesteps=3140000, episode_reward=-191.49 +/- 157.24\n",
      "Episode length: 2043.00 +/- 843.95\n",
      "Eval num_timesteps=3145000, episode_reward=-306.31 +/- 24.16\n",
      "Episode length: 1782.80 +/- 680.18\n",
      "Eval num_timesteps=3150000, episode_reward=-150.32 +/- 145.99\n",
      "Episode length: 2870.00 +/- 171.09\n",
      "Eval num_timesteps=3155000, episode_reward=-236.02 +/- 160.39\n",
      "Episode length: 1698.40 +/- 1062.99\n",
      "Eval num_timesteps=3160000, episode_reward=-358.93 +/- 47.58\n",
      "Episode length: 1774.80 +/- 819.09\n",
      "Eval num_timesteps=3165000, episode_reward=-248.27 +/- 125.12\n",
      "Episode length: 1829.00 +/- 837.56\n",
      "Eval num_timesteps=3170000, episode_reward=-71.39 +/- 114.17\n",
      "Episode length: 2581.60 +/- 836.80\n",
      "Eval num_timesteps=3175000, episode_reward=-294.46 +/- 3.93\n",
      "Episode length: 1149.60 +/- 278.18\n",
      "Eval num_timesteps=3180000, episode_reward=-394.54 +/- 57.84\n",
      "Episode length: 1194.40 +/- 218.51\n",
      "Eval num_timesteps=3185000, episode_reward=-366.31 +/- 51.98\n",
      "Episode length: 1201.40 +/- 364.72\n",
      "Eval num_timesteps=3190000, episode_reward=-403.74 +/- 15.95\n",
      "Episode length: 816.20 +/- 210.77\n",
      "Eval num_timesteps=3195000, episode_reward=-397.85 +/- 59.45\n",
      "Episode length: 921.20 +/- 441.50\n",
      "Eval num_timesteps=3200000, episode_reward=-348.27 +/- 5.91\n",
      "Episode length: 908.20 +/- 183.84\n",
      "Eval num_timesteps=3205000, episode_reward=-343.88 +/- 3.04\n",
      "Episode length: 1648.60 +/- 457.59\n",
      "Eval num_timesteps=3210000, episode_reward=-339.67 +/- 11.22\n",
      "Episode length: 1630.20 +/- 686.11\n",
      "Eval num_timesteps=3215000, episode_reward=-353.69 +/- 3.10\n",
      "Episode length: 1301.80 +/- 278.05\n",
      "Eval num_timesteps=3220000, episode_reward=-445.76 +/- 290.55\n",
      "Episode length: 1794.60 +/- 711.42\n",
      "Eval num_timesteps=3225000, episode_reward=-349.59 +/- 9.12\n",
      "Episode length: 1566.20 +/- 583.65\n",
      "Eval num_timesteps=3230000, episode_reward=-344.04 +/- 12.54\n",
      "Episode length: 1729.60 +/- 422.35\n",
      "Eval num_timesteps=3235000, episode_reward=-394.86 +/- 51.42\n",
      "Episode length: 1659.40 +/- 385.07\n",
      "Eval num_timesteps=3240000, episode_reward=-152.41 +/- 10.42\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3245000, episode_reward=-192.32 +/- 160.50\n",
      "Episode length: 2615.80 +/- 768.40\n",
      "Eval num_timesteps=3250000, episode_reward=-130.28 +/- 88.50\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3255000, episode_reward=-296.63 +/- 128.78\n",
      "Episode length: 1881.80 +/- 781.34\n",
      "Eval num_timesteps=3260000, episode_reward=-309.13 +/- 31.01\n",
      "Episode length: 1092.20 +/- 417.62\n",
      "Eval num_timesteps=3265000, episode_reward=-292.80 +/- 7.27\n",
      "Episode length: 951.20 +/- 209.56\n",
      "Eval num_timesteps=3270000, episode_reward=-306.06 +/- 8.17\n",
      "Episode length: 816.20 +/- 110.06\n",
      "Eval num_timesteps=3275000, episode_reward=-300.95 +/- 26.81\n",
      "Episode length: 1366.40 +/- 190.71\n",
      "Eval num_timesteps=3280000, episode_reward=-331.62 +/- 32.06\n",
      "Episode length: 1358.20 +/- 638.91\n",
      "Eval num_timesteps=3285000, episode_reward=-658.10 +/- 401.26\n",
      "Episode length: 2116.00 +/- 543.64\n",
      "Eval num_timesteps=3290000, episode_reward=-547.22 +/- 274.08\n",
      "Episode length: 1642.00 +/- 560.54\n",
      "Eval num_timesteps=3295000, episode_reward=-386.78 +/- 56.29\n",
      "Episode length: 1747.40 +/- 744.90\n",
      "Eval num_timesteps=3300000, episode_reward=-105.14 +/- 137.94\n",
      "Episode length: 2748.80 +/- 502.40\n",
      "Eval num_timesteps=3305000, episode_reward=-176.37 +/- 247.52\n",
      "Episode length: 2721.20 +/- 557.60\n",
      "Eval num_timesteps=3310000, episode_reward=-237.19 +/- 118.66\n",
      "Episode length: 2018.00 +/- 810.54\n",
      "Eval num_timesteps=3315000, episode_reward=-260.70 +/- 135.08\n",
      "Episode length: 1999.20 +/- 795.10\n",
      "Eval num_timesteps=3320000, episode_reward=-274.85 +/- 341.91\n",
      "Episode length: 2617.00 +/- 766.00\n",
      "Eval num_timesteps=3325000, episode_reward=-360.59 +/- 336.31\n",
      "Episode length: 1839.40 +/- 961.80\n",
      "Eval num_timesteps=3330000, episode_reward=-422.01 +/- 56.01\n",
      "Episode length: 1192.00 +/- 478.62\n",
      "Eval num_timesteps=3335000, episode_reward=-135.52 +/- 143.86\n",
      "Episode length: 2324.00 +/- 896.09\n",
      "Eval num_timesteps=3340000, episode_reward=-287.03 +/- 298.19\n",
      "Episode length: 1857.00 +/- 937.68\n",
      "Eval num_timesteps=3345000, episode_reward=-302.65 +/- 12.47\n",
      "Episode length: 2059.80 +/- 528.68\n",
      "Eval num_timesteps=3350000, episode_reward=-482.99 +/- 254.95\n",
      "Episode length: 1508.60 +/- 560.06\n",
      "Eval num_timesteps=3355000, episode_reward=-340.89 +/- 319.59\n",
      "Episode length: 2083.40 +/- 800.02\n",
      "Eval num_timesteps=3360000, episode_reward=-664.12 +/- 193.94\n",
      "Episode length: 1670.80 +/- 835.02\n",
      "Eval num_timesteps=3365000, episode_reward=-628.30 +/- 295.48\n",
      "Episode length: 1356.00 +/- 562.47\n",
      "Eval num_timesteps=3370000, episode_reward=-403.43 +/- 207.09\n",
      "Episode length: 1240.00 +/- 447.61\n",
      "Eval num_timesteps=3375000, episode_reward=-362.98 +/- 219.76\n",
      "Episode length: 1500.20 +/- 902.45\n",
      "Eval num_timesteps=3380000, episode_reward=-289.95 +/- 248.61\n",
      "Episode length: 2640.60 +/- 496.01\n",
      "Eval num_timesteps=3385000, episode_reward=-239.01 +/- 152.10\n",
      "Episode length: 1978.40 +/- 880.01\n",
      "Eval num_timesteps=3390000, episode_reward=-34.42 +/- 40.78\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3395000, episode_reward=-307.47 +/- 83.48\n",
      "Episode length: 1935.40 +/- 574.47\n",
      "Eval num_timesteps=3400000, episode_reward=-215.01 +/- 175.19\n",
      "Episode length: 1960.20 +/- 911.68\n",
      "Eval num_timesteps=3405000, episode_reward=-382.03 +/- 26.80\n",
      "Episode length: 1419.00 +/- 399.23\n",
      "Eval num_timesteps=3410000, episode_reward=-332.52 +/- 28.45\n",
      "Episode length: 1072.00 +/- 214.59\n",
      "Eval num_timesteps=3415000, episode_reward=-338.22 +/- 19.27\n",
      "Episode length: 1168.20 +/- 202.59\n",
      "Eval num_timesteps=3420000, episode_reward=-334.51 +/- 28.45\n",
      "Episode length: 967.00 +/- 210.57\n",
      "Eval num_timesteps=3425000, episode_reward=-369.41 +/- 4.13\n",
      "Episode length: 828.00 +/- 139.38\n",
      "Eval num_timesteps=3430000, episode_reward=-407.80 +/- 14.26\n",
      "Episode length: 1113.40 +/- 140.69\n",
      "Eval num_timesteps=3435000, episode_reward=-403.29 +/- 20.12\n",
      "Episode length: 857.80 +/- 143.97\n",
      "Eval num_timesteps=3440000, episode_reward=-333.26 +/- 4.30\n",
      "Episode length: 846.80 +/- 90.60\n",
      "Eval num_timesteps=3445000, episode_reward=-309.24 +/- 6.80\n",
      "Episode length: 814.60 +/- 110.88\n",
      "Eval num_timesteps=3450000, episode_reward=-463.73 +/- 24.15\n",
      "Episode length: 1064.80 +/- 52.44\n",
      "Eval num_timesteps=3455000, episode_reward=-518.00 +/- 24.21\n",
      "Episode length: 1044.00 +/- 64.02\n",
      "Eval num_timesteps=3460000, episode_reward=-289.33 +/- 4.30\n",
      "Episode length: 973.20 +/- 108.16\n",
      "Eval num_timesteps=3465000, episode_reward=-290.53 +/- 3.22\n",
      "Episode length: 1048.60 +/- 188.60\n",
      "Eval num_timesteps=3470000, episode_reward=-290.59 +/- 21.96\n",
      "Episode length: 989.40 +/- 196.16\n",
      "Eval num_timesteps=3475000, episode_reward=-487.02 +/- 38.16\n",
      "Episode length: 997.20 +/- 170.30\n",
      "Eval num_timesteps=3480000, episode_reward=-650.33 +/- 183.92\n",
      "Episode length: 1886.00 +/- 618.45\n",
      "Eval num_timesteps=3485000, episode_reward=-653.95 +/- 224.12\n",
      "Episode length: 1651.00 +/- 345.71\n",
      "Eval num_timesteps=3490000, episode_reward=-740.49 +/- 134.34\n",
      "Episode length: 1183.60 +/- 335.55\n",
      "Eval num_timesteps=3495000, episode_reward=-499.96 +/- 198.11\n",
      "Episode length: 1057.00 +/- 295.60\n",
      "Eval num_timesteps=3500000, episode_reward=-893.98 +/- 74.09\n",
      "Episode length: 944.00 +/- 173.86\n",
      "Eval num_timesteps=3505000, episode_reward=-540.74 +/- 62.40\n",
      "Episode length: 921.40 +/- 189.61\n",
      "Eval num_timesteps=3510000, episode_reward=-571.40 +/- 76.62\n",
      "Episode length: 1184.00 +/- 259.73\n",
      "Eval num_timesteps=3515000, episode_reward=-273.76 +/- 7.53\n",
      "Episode length: 1100.00 +/- 86.74\n",
      "Eval num_timesteps=3520000, episode_reward=-820.09 +/- 134.05\n",
      "Episode length: 979.20 +/- 176.03\n",
      "Eval num_timesteps=3525000, episode_reward=-288.07 +/- 9.03\n",
      "Episode length: 1124.80 +/- 288.47\n",
      "Eval num_timesteps=3530000, episode_reward=-728.99 +/- 137.64\n",
      "Episode length: 1234.80 +/- 380.53\n",
      "Eval num_timesteps=3535000, episode_reward=-466.26 +/- 11.57\n",
      "Episode length: 1401.00 +/- 121.51\n",
      "Eval num_timesteps=3540000, episode_reward=-811.29 +/- 142.47\n",
      "Episode length: 1482.20 +/- 362.84\n",
      "Eval num_timesteps=3545000, episode_reward=-460.40 +/- 24.62\n",
      "Episode length: 2119.40 +/- 276.91\n",
      "Eval num_timesteps=3550000, episode_reward=-590.74 +/- 220.03\n",
      "Episode length: 2643.40 +/- 352.98\n",
      "Eval num_timesteps=3555000, episode_reward=-785.72 +/- 69.45\n",
      "Episode length: 846.60 +/- 23.99\n",
      "Eval num_timesteps=3560000, episode_reward=-855.77 +/- 86.54\n",
      "Episode length: 811.40 +/- 35.40\n",
      "Eval num_timesteps=3565000, episode_reward=-566.08 +/- 439.29\n",
      "Episode length: 2936.00 +/- 78.75\n",
      "Eval num_timesteps=3570000, episode_reward=-428.61 +/- 291.89\n",
      "Episode length: 2905.60 +/- 115.66\n",
      "Eval num_timesteps=3575000, episode_reward=-161.20 +/- 35.39\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3580000, episode_reward=-149.39 +/- 34.69\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3585000, episode_reward=-141.48 +/- 8.29\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3590000, episode_reward=-166.90 +/- 40.17\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3595000, episode_reward=-259.56 +/- 31.94\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3600000, episode_reward=-327.99 +/- 93.20\n",
      "Episode length: 2880.40 +/- 239.20\n",
      "Eval num_timesteps=3605000, episode_reward=-305.32 +/- 18.72\n",
      "Episode length: 836.80 +/- 217.31\n",
      "Eval num_timesteps=3610000, episode_reward=-525.41 +/- 301.33\n",
      "Episode length: 2012.40 +/- 1003.47\n",
      "Eval num_timesteps=3615000, episode_reward=-379.81 +/- 115.49\n",
      "Episode length: 2192.40 +/- 834.78\n",
      "Eval num_timesteps=3620000, episode_reward=-286.39 +/- 33.83\n",
      "Episode length: 1250.00 +/- 885.02\n",
      "Eval num_timesteps=3625000, episode_reward=-425.59 +/- 82.76\n",
      "Episode length: 1957.60 +/- 1026.51\n",
      "Eval num_timesteps=3630000, episode_reward=-89.43 +/- 26.90\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3635000, episode_reward=-101.62 +/- 35.64\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3640000, episode_reward=-92.55 +/- 8.35\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3645000, episode_reward=-303.47 +/- 6.65\n",
      "Episode length: 830.40 +/- 114.83\n",
      "Eval num_timesteps=3650000, episode_reward=-316.00 +/- 81.99\n",
      "Episode length: 2162.60 +/- 1014.71\n",
      "Eval num_timesteps=3655000, episode_reward=-315.74 +/- 6.04\n",
      "Episode length: 1221.80 +/- 246.57\n",
      "Eval num_timesteps=3660000, episode_reward=-317.79 +/- 13.31\n",
      "Episode length: 1116.40 +/- 397.41\n",
      "Eval num_timesteps=3665000, episode_reward=-281.35 +/- 129.82\n",
      "Episode length: 1718.20 +/- 730.76\n",
      "Eval num_timesteps=3670000, episode_reward=-325.55 +/- 72.38\n",
      "Episode length: 1898.60 +/- 619.19\n",
      "Eval num_timesteps=3675000, episode_reward=-23.38 +/- 18.96\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3680000, episode_reward=-176.87 +/- 189.81\n",
      "Episode length: 2781.80 +/- 436.40\n",
      "Eval num_timesteps=3685000, episode_reward=-139.58 +/- 125.11\n",
      "Episode length: 2619.60 +/- 760.80\n",
      "Eval num_timesteps=3690000, episode_reward=-117.87 +/- 103.79\n",
      "Episode length: 2738.60 +/- 522.80\n",
      "Eval num_timesteps=3695000, episode_reward=-176.12 +/- 133.66\n",
      "Episode length: 2812.40 +/- 231.84\n",
      "Eval num_timesteps=3700000, episode_reward=-182.82 +/- 85.09\n",
      "Episode length: 2694.40 +/- 611.20\n",
      "Eval num_timesteps=3705000, episode_reward=-125.85 +/- 131.14\n",
      "Episode length: 2838.80 +/- 322.40\n",
      "Eval num_timesteps=3710000, episode_reward=-278.77 +/- 129.32\n",
      "Episode length: 1487.40 +/- 772.14\n",
      "Eval num_timesteps=3715000, episode_reward=-295.53 +/- 96.09\n",
      "Episode length: 1639.60 +/- 728.18\n",
      "Eval num_timesteps=3720000, episode_reward=-356.23 +/- 17.19\n",
      "Episode length: 1863.60 +/- 369.80\n",
      "Eval num_timesteps=3725000, episode_reward=-342.46 +/- 8.61\n",
      "Episode length: 1462.80 +/- 538.29\n",
      "Eval num_timesteps=3730000, episode_reward=-228.00 +/- 122.05\n",
      "Episode length: 2210.60 +/- 674.64\n",
      "Eval num_timesteps=3735000, episode_reward=-302.02 +/- 142.38\n",
      "Episode length: 1892.80 +/- 878.23\n",
      "Eval num_timesteps=3740000, episode_reward=-311.64 +/- 157.62\n",
      "Episode length: 2061.80 +/- 928.87\n",
      "Eval num_timesteps=3745000, episode_reward=-360.90 +/- 139.04\n",
      "Episode length: 1554.40 +/- 741.62\n",
      "Eval num_timesteps=3750000, episode_reward=-377.95 +/- 157.43\n",
      "Episode length: 1896.40 +/- 701.39\n",
      "Eval num_timesteps=3755000, episode_reward=-502.24 +/- 213.42\n",
      "Episode length: 1902.80 +/- 827.61\n",
      "Eval num_timesteps=3760000, episode_reward=-492.98 +/- 65.94\n",
      "Episode length: 1126.60 +/- 240.92\n",
      "Eval num_timesteps=3765000, episode_reward=-485.95 +/- 17.20\n",
      "Episode length: 1354.20 +/- 526.42\n",
      "Eval num_timesteps=3770000, episode_reward=-427.00 +/- 49.26\n",
      "Episode length: 1288.40 +/- 409.88\n",
      "Eval num_timesteps=3775000, episode_reward=-393.37 +/- 25.52\n",
      "Episode length: 1235.40 +/- 274.22\n",
      "Eval num_timesteps=3780000, episode_reward=-339.88 +/- 9.30\n",
      "Episode length: 1765.40 +/- 214.61\n",
      "Eval num_timesteps=3785000, episode_reward=-330.85 +/- 22.10\n",
      "Episode length: 1742.80 +/- 444.17\n",
      "Eval num_timesteps=3790000, episode_reward=-383.37 +/- 27.29\n",
      "Episode length: 1713.80 +/- 475.53\n",
      "Eval num_timesteps=3795000, episode_reward=-331.42 +/- 20.19\n",
      "Episode length: 1488.00 +/- 521.73\n",
      "Eval num_timesteps=3800000, episode_reward=-305.68 +/- 9.69\n",
      "Episode length: 2015.20 +/- 256.13\n",
      "Eval num_timesteps=3805000, episode_reward=-342.85 +/- 23.11\n",
      "Episode length: 1482.40 +/- 751.32\n",
      "Eval num_timesteps=3810000, episode_reward=-314.73 +/- 32.29\n",
      "Episode length: 1686.20 +/- 186.97\n",
      "Eval num_timesteps=3815000, episode_reward=-308.02 +/- 15.83\n",
      "Episode length: 1486.80 +/- 274.51\n",
      "Eval num_timesteps=3820000, episode_reward=-444.30 +/- 108.92\n",
      "Episode length: 1169.40 +/- 385.60\n",
      "Eval num_timesteps=3825000, episode_reward=-303.40 +/- 35.89\n",
      "Episode length: 1222.60 +/- 295.21\n",
      "Eval num_timesteps=3830000, episode_reward=-462.88 +/- 146.46\n",
      "Episode length: 2252.20 +/- 757.29\n",
      "Eval num_timesteps=3835000, episode_reward=-222.40 +/- 106.62\n",
      "Episode length: 1778.20 +/- 1000.34\n",
      "Eval num_timesteps=3840000, episode_reward=-281.66 +/- 69.67\n",
      "Episode length: 1346.80 +/- 874.34\n",
      "Eval num_timesteps=3845000, episode_reward=-553.86 +/- 61.15\n",
      "Episode length: 971.40 +/- 588.03\n",
      "Eval num_timesteps=3850000, episode_reward=-269.95 +/- 140.13\n",
      "Episode length: 1742.80 +/- 1040.73\n",
      "Eval num_timesteps=3855000, episode_reward=-248.49 +/- 84.05\n",
      "Episode length: 1309.20 +/- 854.94\n",
      "Eval num_timesteps=3860000, episode_reward=-294.44 +/- 9.06\n",
      "Episode length: 976.60 +/- 204.47\n",
      "Eval num_timesteps=3865000, episode_reward=-280.54 +/- 6.85\n",
      "Episode length: 983.60 +/- 276.49\n",
      "Eval num_timesteps=3870000, episode_reward=-273.33 +/- 12.40\n",
      "Episode length: 1229.80 +/- 326.31\n",
      "Eval num_timesteps=3875000, episode_reward=-244.96 +/- 108.82\n",
      "Episode length: 2011.80 +/- 821.43\n",
      "Eval num_timesteps=3880000, episode_reward=-272.57 +/- 9.37\n",
      "Episode length: 976.40 +/- 221.90\n",
      "Eval num_timesteps=3885000, episode_reward=-272.96 +/- 4.53\n",
      "Episode length: 884.00 +/- 342.51\n",
      "Eval num_timesteps=3890000, episode_reward=-292.27 +/- 6.71\n",
      "Episode length: 828.80 +/- 216.08\n",
      "Eval num_timesteps=3895000, episode_reward=-350.92 +/- 26.55\n",
      "Episode length: 884.60 +/- 242.24\n",
      "Eval num_timesteps=3900000, episode_reward=-310.33 +/- 6.18\n",
      "Episode length: 774.20 +/- 229.24\n",
      "Eval num_timesteps=3905000, episode_reward=-284.22 +/- 9.52\n",
      "Episode length: 901.80 +/- 346.71\n",
      "Eval num_timesteps=3910000, episode_reward=-272.72 +/- 6.05\n",
      "Episode length: 812.80 +/- 245.92\n",
      "Eval num_timesteps=3915000, episode_reward=-404.43 +/- 26.73\n",
      "Episode length: 860.60 +/- 314.42\n",
      "Eval num_timesteps=3920000, episode_reward=-308.91 +/- 26.23\n",
      "Episode length: 1072.60 +/- 512.13\n",
      "Eval num_timesteps=3925000, episode_reward=-366.03 +/- 13.97\n",
      "Episode length: 1008.20 +/- 255.74\n",
      "Eval num_timesteps=3930000, episode_reward=-393.81 +/- 4.86\n",
      "Episode length: 757.60 +/- 157.84\n",
      "Eval num_timesteps=3935000, episode_reward=-364.48 +/- 6.81\n",
      "Episode length: 910.00 +/- 218.27\n",
      "Eval num_timesteps=3940000, episode_reward=-394.69 +/- 8.03\n",
      "Episode length: 708.20 +/- 136.13\n",
      "Eval num_timesteps=3945000, episode_reward=-420.04 +/- 26.86\n",
      "Episode length: 730.80 +/- 202.20\n",
      "Eval num_timesteps=3950000, episode_reward=-406.43 +/- 9.37\n",
      "Episode length: 646.00 +/- 139.63\n",
      "Eval num_timesteps=3955000, episode_reward=-297.32 +/- 4.85\n",
      "Episode length: 831.80 +/- 143.11\n",
      "Eval num_timesteps=3960000, episode_reward=-302.54 +/- 8.99\n",
      "Episode length: 795.20 +/- 158.46\n",
      "Eval num_timesteps=3965000, episode_reward=-447.06 +/- 13.90\n",
      "Episode length: 1252.20 +/- 61.30\n",
      "Eval num_timesteps=3970000, episode_reward=-186.77 +/- 13.48\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3975000, episode_reward=-224.51 +/- 56.57\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=3980000, episode_reward=-337.90 +/- 326.59\n",
      "Episode length: 2965.80 +/- 68.40\n",
      "Eval num_timesteps=3985000, episode_reward=-264.32 +/- 107.64\n",
      "Episode length: 1741.60 +/- 984.84\n",
      "Eval num_timesteps=3990000, episode_reward=-320.82 +/- 22.10\n",
      "Episode length: 732.00 +/- 119.35\n",
      "Eval num_timesteps=3995000, episode_reward=-328.25 +/- 11.65\n",
      "Episode length: 909.60 +/- 293.08\n",
      "Eval num_timesteps=4000000, episode_reward=-284.41 +/- 10.13\n",
      "Episode length: 815.80 +/- 171.50\n",
      "Eval num_timesteps=4005000, episode_reward=-335.11 +/- 161.96\n",
      "Episode length: 1557.40 +/- 764.01\n",
      "Eval num_timesteps=4010000, episode_reward=-379.00 +/- 131.01\n",
      "Episode length: 2031.00 +/- 1033.61\n",
      "Eval num_timesteps=4015000, episode_reward=-347.42 +/- 40.12\n",
      "Episode length: 1371.40 +/- 444.47\n",
      "Eval num_timesteps=4020000, episode_reward=-301.60 +/- 32.15\n",
      "Episode length: 1345.80 +/- 187.91\n",
      "Eval num_timesteps=4025000, episode_reward=-696.48 +/- 196.56\n",
      "Episode length: 1372.20 +/- 292.14\n",
      "Eval num_timesteps=4030000, episode_reward=-303.29 +/- 14.37\n",
      "Episode length: 919.60 +/- 199.51\n",
      "Eval num_timesteps=4035000, episode_reward=-370.14 +/- 229.85\n",
      "Episode length: 2757.00 +/- 301.62\n",
      "Eval num_timesteps=4040000, episode_reward=-531.44 +/- 84.18\n",
      "Episode length: 1757.20 +/- 713.26\n",
      "Eval num_timesteps=4045000, episode_reward=-339.03 +/- 13.62\n",
      "Episode length: 788.00 +/- 82.94\n",
      "Eval num_timesteps=4050000, episode_reward=-466.92 +/- 13.86\n",
      "Episode length: 840.20 +/- 86.86\n",
      "Eval num_timesteps=4055000, episode_reward=-350.63 +/- 25.83\n",
      "Episode length: 818.80 +/- 70.76\n",
      "Eval num_timesteps=4060000, episode_reward=-448.16 +/- 89.77\n",
      "Episode length: 908.40 +/- 90.14\n",
      "Eval num_timesteps=4065000, episode_reward=-320.98 +/- 7.58\n",
      "Episode length: 996.80 +/- 12.20\n",
      "Eval num_timesteps=4070000, episode_reward=-385.90 +/- 14.16\n",
      "Episode length: 859.40 +/- 53.57\n",
      "Eval num_timesteps=4075000, episode_reward=-558.27 +/- 138.16\n",
      "Episode length: 904.60 +/- 100.13\n",
      "Eval num_timesteps=4080000, episode_reward=-159.30 +/- 27.26\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4085000, episode_reward=-417.69 +/- 139.83\n",
      "Episode length: 2613.80 +/- 770.40\n",
      "Eval num_timesteps=4090000, episode_reward=-369.65 +/- 5.41\n",
      "Episode length: 860.20 +/- 77.58\n",
      "Eval num_timesteps=4095000, episode_reward=-352.64 +/- 9.29\n",
      "Episode length: 987.80 +/- 39.93\n",
      "Eval num_timesteps=4100000, episode_reward=-321.27 +/- 57.36\n",
      "Episode length: 1418.60 +/- 793.37\n",
      "Eval num_timesteps=4105000, episode_reward=-245.26 +/- 205.49\n",
      "Episode length: 2868.00 +/- 162.07\n",
      "Eval num_timesteps=4110000, episode_reward=-81.40 +/- 75.94\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4115000, episode_reward=-366.04 +/- 8.57\n",
      "Episode length: 1009.80 +/- 150.94\n",
      "Eval num_timesteps=4120000, episode_reward=-353.04 +/- 10.95\n",
      "Episode length: 1271.00 +/- 354.71\n",
      "Eval num_timesteps=4125000, episode_reward=-343.59 +/- 9.91\n",
      "Episode length: 1902.40 +/- 305.44\n",
      "Eval num_timesteps=4130000, episode_reward=-223.15 +/- 169.83\n",
      "Episode length: 2574.40 +/- 539.67\n",
      "Eval num_timesteps=4135000, episode_reward=-24.26 +/- 11.23\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4140000, episode_reward=-290.20 +/- 143.25\n",
      "Episode length: 2000.80 +/- 647.53\n",
      "Eval num_timesteps=4145000, episode_reward=-354.49 +/- 6.66\n",
      "Episode length: 1489.00 +/- 470.25\n",
      "Eval num_timesteps=4150000, episode_reward=-244.82 +/- 144.22\n",
      "Episode length: 2373.20 +/- 756.13\n",
      "Eval num_timesteps=4155000, episode_reward=-217.41 +/- 109.56\n",
      "Episode length: 2178.80 +/- 1006.55\n",
      "Eval num_timesteps=4160000, episode_reward=-400.38 +/- 41.29\n",
      "Episode length: 1754.40 +/- 686.53\n",
      "Eval num_timesteps=4165000, episode_reward=-362.27 +/- 6.24\n",
      "Episode length: 1806.40 +/- 433.49\n",
      "Eval num_timesteps=4170000, episode_reward=-254.03 +/- 149.05\n",
      "Episode length: 1919.60 +/- 924.91\n",
      "Eval num_timesteps=4175000, episode_reward=-303.45 +/- 126.30\n",
      "Episode length: 1352.80 +/- 839.28\n",
      "Eval num_timesteps=4180000, episode_reward=-491.69 +/- 17.46\n",
      "Episode length: 1252.40 +/- 264.99\n",
      "Eval num_timesteps=4185000, episode_reward=-440.81 +/- 25.88\n",
      "Episode length: 1098.60 +/- 362.38\n",
      "Eval num_timesteps=4190000, episode_reward=-349.54 +/- 115.34\n",
      "Episode length: 1550.80 +/- 841.71\n",
      "Eval num_timesteps=4195000, episode_reward=-41.64 +/- 29.08\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4200000, episode_reward=-278.38 +/- 176.48\n",
      "Episode length: 2463.40 +/- 519.02\n",
      "Eval num_timesteps=4205000, episode_reward=-123.75 +/- 145.45\n",
      "Episode length: 2591.80 +/- 816.40\n",
      "Eval num_timesteps=4210000, episode_reward=-240.48 +/- 204.75\n",
      "Episode length: 2546.20 +/- 699.86\n",
      "Eval num_timesteps=4215000, episode_reward=-130.96 +/- 153.79\n",
      "Episode length: 2880.40 +/- 239.20\n",
      "Eval num_timesteps=4220000, episode_reward=-365.27 +/- 179.64\n",
      "Episode length: 2064.40 +/- 754.23\n",
      "Eval num_timesteps=4225000, episode_reward=-387.25 +/- 150.45\n",
      "Episode length: 1605.40 +/- 914.38\n",
      "Eval num_timesteps=4230000, episode_reward=-344.31 +/- 197.58\n",
      "Episode length: 1923.80 +/- 966.82\n",
      "Eval num_timesteps=4235000, episode_reward=-474.21 +/- 166.64\n",
      "Episode length: 1662.00 +/- 724.90\n",
      "Eval num_timesteps=4240000, episode_reward=-503.16 +/- 30.72\n",
      "Episode length: 2054.80 +/- 624.04\n",
      "Eval num_timesteps=4245000, episode_reward=-534.70 +/- 31.40\n",
      "Episode length: 1719.20 +/- 287.76\n",
      "Eval num_timesteps=4250000, episode_reward=-475.71 +/- 73.54\n",
      "Episode length: 1378.00 +/- 416.08\n",
      "Eval num_timesteps=4255000, episode_reward=-399.66 +/- 87.97\n",
      "Episode length: 1573.40 +/- 633.09\n",
      "Eval num_timesteps=4260000, episode_reward=-394.67 +/- 119.45\n",
      "Episode length: 951.60 +/- 111.33\n",
      "Eval num_timesteps=4265000, episode_reward=-439.58 +/- 170.36\n",
      "Episode length: 957.80 +/- 240.45\n",
      "Eval num_timesteps=4270000, episode_reward=-628.84 +/- 241.35\n",
      "Episode length: 1463.20 +/- 712.93\n",
      "Eval num_timesteps=4275000, episode_reward=-625.44 +/- 162.89\n",
      "Episode length: 1200.20 +/- 394.74\n",
      "Eval num_timesteps=4280000, episode_reward=-666.01 +/- 34.03\n",
      "Episode length: 1410.40 +/- 215.94\n",
      "Eval num_timesteps=4285000, episode_reward=-611.63 +/- 237.78\n",
      "Episode length: 1172.20 +/- 156.52\n",
      "Eval num_timesteps=4290000, episode_reward=-761.39 +/- 6.80\n",
      "Episode length: 962.20 +/- 144.98\n",
      "Eval num_timesteps=4295000, episode_reward=-618.66 +/- 153.28\n",
      "Episode length: 887.40 +/- 240.99\n",
      "Eval num_timesteps=4300000, episode_reward=-729.17 +/- 192.18\n",
      "Episode length: 1056.00 +/- 366.28\n",
      "Eval num_timesteps=4305000, episode_reward=-740.68 +/- 9.25\n",
      "Episode length: 1032.80 +/- 180.29\n",
      "Eval num_timesteps=4310000, episode_reward=-719.03 +/- 7.08\n",
      "Episode length: 1104.60 +/- 56.30\n",
      "Eval num_timesteps=4315000, episode_reward=-631.62 +/- 162.02\n",
      "Episode length: 1005.80 +/- 109.62\n",
      "Eval num_timesteps=4320000, episode_reward=-562.68 +/- 8.50\n",
      "Episode length: 943.60 +/- 196.54\n",
      "Eval num_timesteps=4325000, episode_reward=-509.08 +/- 12.01\n",
      "Episode length: 1163.60 +/- 235.67\n",
      "Eval num_timesteps=4330000, episode_reward=-361.34 +/- 11.17\n",
      "Episode length: 1022.20 +/- 449.93\n",
      "Eval num_timesteps=4335000, episode_reward=-340.20 +/- 22.17\n",
      "Episode length: 1352.40 +/- 600.36\n",
      "Eval num_timesteps=4340000, episode_reward=-316.56 +/- 6.27\n",
      "Episode length: 1499.80 +/- 607.91\n",
      "Eval num_timesteps=4345000, episode_reward=-481.10 +/- 304.79\n",
      "Episode length: 1754.60 +/- 860.36\n",
      "Eval num_timesteps=4350000, episode_reward=-300.12 +/- 8.90\n",
      "Episode length: 1196.40 +/- 204.79\n",
      "Eval num_timesteps=4355000, episode_reward=-305.79 +/- 4.25\n",
      "Episode length: 932.00 +/- 228.97\n",
      "Eval num_timesteps=4360000, episode_reward=-304.01 +/- 6.98\n",
      "Episode length: 1080.20 +/- 365.17\n",
      "Eval num_timesteps=4365000, episode_reward=-296.92 +/- 2.48\n",
      "Episode length: 1019.20 +/- 233.78\n",
      "Eval num_timesteps=4370000, episode_reward=-331.35 +/- 59.87\n",
      "Episode length: 1077.40 +/- 274.95\n",
      "Eval num_timesteps=4375000, episode_reward=-300.57 +/- 7.29\n",
      "Episode length: 936.80 +/- 232.52\n",
      "Eval num_timesteps=4380000, episode_reward=-310.63 +/- 7.76\n",
      "Episode length: 936.60 +/- 347.96\n",
      "Eval num_timesteps=4385000, episode_reward=-338.35 +/- 47.19\n",
      "Episode length: 1276.80 +/- 121.90\n",
      "Eval num_timesteps=4390000, episode_reward=-317.99 +/- 12.24\n",
      "Episode length: 1551.40 +/- 360.10\n",
      "Eval num_timesteps=4395000, episode_reward=-320.98 +/- 18.29\n",
      "Episode length: 1129.00 +/- 273.60\n",
      "Eval num_timesteps=4400000, episode_reward=-358.96 +/- 86.76\n",
      "Episode length: 1267.60 +/- 297.66\n",
      "Eval num_timesteps=4405000, episode_reward=-289.41 +/- 13.49\n",
      "Episode length: 943.80 +/- 385.60\n",
      "Eval num_timesteps=4410000, episode_reward=-295.11 +/- 9.43\n",
      "Episode length: 1174.80 +/- 368.65\n",
      "Eval num_timesteps=4415000, episode_reward=-355.25 +/- 55.43\n",
      "Episode length: 1479.60 +/- 242.58\n",
      "Eval num_timesteps=4420000, episode_reward=-293.16 +/- 115.19\n",
      "Episode length: 1641.40 +/- 777.25\n",
      "Eval num_timesteps=4425000, episode_reward=-325.48 +/- 20.44\n",
      "Episode length: 1180.80 +/- 536.78\n",
      "Eval num_timesteps=4430000, episode_reward=-317.97 +/- 10.75\n",
      "Episode length: 785.60 +/- 159.29\n",
      "Eval num_timesteps=4435000, episode_reward=-555.93 +/- 51.09\n",
      "Episode length: 841.80 +/- 320.97\n",
      "Eval num_timesteps=4440000, episode_reward=-514.35 +/- 17.88\n",
      "Episode length: 963.80 +/- 198.29\n",
      "Eval num_timesteps=4445000, episode_reward=-341.72 +/- 8.87\n",
      "Episode length: 1022.20 +/- 268.77\n",
      "Eval num_timesteps=4450000, episode_reward=-366.45 +/- 44.79\n",
      "Episode length: 1117.60 +/- 447.62\n",
      "Eval num_timesteps=4455000, episode_reward=-489.66 +/- 123.77\n",
      "Episode length: 928.80 +/- 194.56\n",
      "Eval num_timesteps=4460000, episode_reward=-401.04 +/- 107.17\n",
      "Episode length: 1587.20 +/- 622.30\n",
      "Eval num_timesteps=4465000, episode_reward=-304.88 +/- 11.28\n",
      "Episode length: 1192.00 +/- 469.33\n",
      "Eval num_timesteps=4470000, episode_reward=-359.84 +/- 89.41\n",
      "Episode length: 1547.40 +/- 779.32\n",
      "Eval num_timesteps=4475000, episode_reward=-217.86 +/- 94.03\n",
      "Episode length: 2692.20 +/- 615.60\n",
      "Eval num_timesteps=4480000, episode_reward=-204.55 +/- 43.03\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4485000, episode_reward=-145.06 +/- 95.29\n",
      "Episode length: 2714.20 +/- 571.60\n",
      "Eval num_timesteps=4490000, episode_reward=-480.35 +/- 86.29\n",
      "Episode length: 1990.80 +/- 769.79\n",
      "Eval num_timesteps=4495000, episode_reward=-326.52 +/- 5.27\n",
      "Episode length: 1317.80 +/- 251.91\n",
      "Eval num_timesteps=4500000, episode_reward=-436.49 +/- 164.04\n",
      "Episode length: 1357.20 +/- 543.01\n",
      "Eval num_timesteps=4505000, episode_reward=-472.63 +/- 185.66\n",
      "Episode length: 1743.00 +/- 749.78\n",
      "Eval num_timesteps=4510000, episode_reward=-509.20 +/- 179.94\n",
      "Episode length: 2166.60 +/- 458.37\n",
      "Eval num_timesteps=4515000, episode_reward=-273.91 +/- 76.49\n",
      "Episode length: 1747.60 +/- 1033.48\n",
      "Eval num_timesteps=4520000, episode_reward=-241.64 +/- 104.24\n",
      "Episode length: 2150.40 +/- 1040.54\n",
      "Eval num_timesteps=4525000, episode_reward=-249.90 +/- 57.96\n",
      "Episode length: 2124.40 +/- 1072.78\n",
      "Eval num_timesteps=4530000, episode_reward=-310.70 +/- 99.49\n",
      "Episode length: 1517.20 +/- 799.19\n",
      "Eval num_timesteps=4535000, episode_reward=-573.86 +/- 252.99\n",
      "Episode length: 1169.40 +/- 438.07\n",
      "Eval num_timesteps=4540000, episode_reward=-401.60 +/- 140.85\n",
      "Episode length: 1062.80 +/- 397.40\n",
      "Eval num_timesteps=4545000, episode_reward=-360.18 +/- 51.49\n",
      "Episode length: 970.00 +/- 342.94\n",
      "Eval num_timesteps=4550000, episode_reward=-390.01 +/- 112.03\n",
      "Episode length: 1129.60 +/- 507.37\n",
      "Eval num_timesteps=4555000, episode_reward=-332.48 +/- 33.29\n",
      "Episode length: 1184.80 +/- 674.76\n",
      "Eval num_timesteps=4560000, episode_reward=-592.15 +/- 257.16\n",
      "Episode length: 1222.60 +/- 523.02\n",
      "Eval num_timesteps=4565000, episode_reward=-548.24 +/- 215.92\n",
      "Episode length: 1369.60 +/- 424.56\n",
      "Eval num_timesteps=4570000, episode_reward=-315.12 +/- 6.15\n",
      "Episode length: 866.80 +/- 212.31\n",
      "Eval num_timesteps=4575000, episode_reward=-126.08 +/- 49.04\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4580000, episode_reward=-407.46 +/- 365.48\n",
      "Episode length: 2218.60 +/- 961.55\n",
      "Eval num_timesteps=4585000, episode_reward=-87.81 +/- 4.26\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4590000, episode_reward=-121.76 +/- 27.59\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4595000, episode_reward=-522.39 +/- 348.69\n",
      "Episode length: 2280.00 +/- 811.82\n",
      "Eval num_timesteps=4600000, episode_reward=-135.53 +/- 38.43\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4605000, episode_reward=-129.27 +/- 41.10\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4610000, episode_reward=-189.40 +/- 29.09\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4615000, episode_reward=-135.92 +/- 70.69\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4620000, episode_reward=-769.62 +/- 332.50\n",
      "Episode length: 1663.80 +/- 879.15\n",
      "Eval num_timesteps=4625000, episode_reward=-139.79 +/- 47.12\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4630000, episode_reward=-748.32 +/- 300.00\n",
      "Episode length: 1682.00 +/- 801.53\n",
      "Eval num_timesteps=4635000, episode_reward=-536.11 +/- 99.46\n",
      "Episode length: 1681.80 +/- 741.57\n",
      "Eval num_timesteps=4640000, episode_reward=-385.71 +/- 30.46\n",
      "Episode length: 1743.00 +/- 472.33\n",
      "Eval num_timesteps=4645000, episode_reward=-301.99 +/- 7.21\n",
      "Episode length: 649.60 +/- 93.32\n",
      "Eval num_timesteps=4650000, episode_reward=-317.64 +/- 42.34\n",
      "Episode length: 1309.80 +/- 855.88\n",
      "Eval num_timesteps=4655000, episode_reward=-358.40 +/- 28.55\n",
      "Episode length: 943.60 +/- 350.98\n",
      "Eval num_timesteps=4660000, episode_reward=-317.48 +/- 18.48\n",
      "Episode length: 958.20 +/- 167.45\n",
      "Eval num_timesteps=4665000, episode_reward=-312.82 +/- 29.65\n",
      "Episode length: 1170.60 +/- 437.39\n",
      "Eval num_timesteps=4670000, episode_reward=-577.79 +/- 191.89\n",
      "Episode length: 1218.80 +/- 329.89\n",
      "Eval num_timesteps=4675000, episode_reward=-535.60 +/- 89.57\n",
      "Episode length: 1216.40 +/- 599.10\n",
      "Eval num_timesteps=4680000, episode_reward=-575.43 +/- 282.71\n",
      "Episode length: 1390.60 +/- 827.70\n",
      "Eval num_timesteps=4685000, episode_reward=-332.53 +/- 270.85\n",
      "Episode length: 2431.20 +/- 790.88\n",
      "Eval num_timesteps=4690000, episode_reward=-146.75 +/- 128.11\n",
      "Episode length: 2694.20 +/- 611.60\n",
      "Eval num_timesteps=4695000, episode_reward=-429.99 +/- 37.88\n",
      "Episode length: 1660.80 +/- 607.25\n",
      "Eval num_timesteps=4700000, episode_reward=-422.52 +/- 11.53\n",
      "Episode length: 1097.20 +/- 324.86\n",
      "Eval num_timesteps=4705000, episode_reward=-416.56 +/- 19.94\n",
      "Episode length: 1238.20 +/- 282.52\n",
      "Eval num_timesteps=4710000, episode_reward=-381.40 +/- 62.59\n",
      "Episode length: 936.60 +/- 156.41\n",
      "Eval num_timesteps=4715000, episode_reward=-395.77 +/- 13.15\n",
      "Episode length: 1539.00 +/- 364.14\n",
      "Eval num_timesteps=4720000, episode_reward=-368.30 +/- 135.54\n",
      "Episode length: 2044.80 +/- 839.57\n",
      "Eval num_timesteps=4725000, episode_reward=-362.22 +/- 91.79\n",
      "Episode length: 1542.60 +/- 861.47\n",
      "Eval num_timesteps=4730000, episode_reward=-337.05 +/- 111.09\n",
      "Episode length: 1576.60 +/- 739.38\n",
      "Eval num_timesteps=4735000, episode_reward=-572.25 +/- 172.23\n",
      "Episode length: 1365.00 +/- 468.74\n",
      "Eval num_timesteps=4740000, episode_reward=-400.30 +/- 49.40\n",
      "Episode length: 1038.40 +/- 149.12\n",
      "Eval num_timesteps=4745000, episode_reward=-382.27 +/- 27.28\n",
      "Episode length: 1055.20 +/- 277.66\n",
      "Eval num_timesteps=4750000, episode_reward=-390.90 +/- 47.73\n",
      "Episode length: 978.20 +/- 154.62\n",
      "Eval num_timesteps=4755000, episode_reward=-348.48 +/- 43.43\n",
      "Episode length: 966.60 +/- 132.12\n",
      "Eval num_timesteps=4760000, episode_reward=-352.25 +/- 55.65\n",
      "Episode length: 1045.20 +/- 241.18\n",
      "Eval num_timesteps=4765000, episode_reward=-208.44 +/- 143.61\n",
      "Episode length: 1960.00 +/- 981.54\n",
      "Eval num_timesteps=4770000, episode_reward=-312.38 +/- 7.66\n",
      "Episode length: 1937.00 +/- 637.57\n",
      "Eval num_timesteps=4775000, episode_reward=-311.73 +/- 19.80\n",
      "Episode length: 1442.60 +/- 656.19\n",
      "Eval num_timesteps=4780000, episode_reward=-331.40 +/- 40.07\n",
      "Episode length: 1542.40 +/- 564.34\n",
      "Eval num_timesteps=4785000, episode_reward=-419.61 +/- 6.35\n",
      "Episode length: 1766.00 +/- 595.75\n",
      "Eval num_timesteps=4790000, episode_reward=-302.74 +/- 5.94\n",
      "Episode length: 1042.20 +/- 283.16\n",
      "Eval num_timesteps=4795000, episode_reward=-340.03 +/- 61.18\n",
      "Episode length: 1115.60 +/- 324.63\n",
      "Eval num_timesteps=4800000, episode_reward=-311.36 +/- 70.96\n",
      "Episode length: 980.60 +/- 153.19\n",
      "Eval num_timesteps=4805000, episode_reward=-283.09 +/- 8.66\n",
      "Episode length: 880.00 +/- 132.47\n",
      "Eval num_timesteps=4810000, episode_reward=-289.68 +/- 11.93\n",
      "Episode length: 1436.60 +/- 643.02\n",
      "Eval num_timesteps=4815000, episode_reward=-329.80 +/- 193.10\n",
      "Episode length: 2984.60 +/- 30.80\n",
      "Eval num_timesteps=4820000, episode_reward=-408.82 +/- 201.03\n",
      "Episode length: 2276.00 +/- 770.35\n",
      "Eval num_timesteps=4825000, episode_reward=-280.57 +/- 187.05\n",
      "Episode length: 2610.60 +/- 771.32\n",
      "Eval num_timesteps=4830000, episode_reward=-603.87 +/- 130.77\n",
      "Episode length: 1729.60 +/- 627.14\n",
      "Eval num_timesteps=4835000, episode_reward=-287.55 +/- 6.04\n",
      "Episode length: 1511.60 +/- 412.55\n",
      "Eval num_timesteps=4840000, episode_reward=-297.16 +/- 9.17\n",
      "Episode length: 1337.40 +/- 342.77\n",
      "Eval num_timesteps=4845000, episode_reward=-517.84 +/- 177.30\n",
      "Episode length: 2461.40 +/- 688.52\n",
      "Eval num_timesteps=4850000, episode_reward=-286.97 +/- 57.81\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4855000, episode_reward=-156.86 +/- 22.55\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4860000, episode_reward=-137.67 +/- 23.49\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4865000, episode_reward=-630.63 +/- 379.94\n",
      "Episode length: 2291.00 +/- 528.18\n",
      "Eval num_timesteps=4870000, episode_reward=-341.44 +/- 13.70\n",
      "Episode length: 2169.20 +/- 205.16\n",
      "Eval num_timesteps=4875000, episode_reward=-284.22 +/- 66.06\n",
      "Episode length: 1692.60 +/- 738.25\n",
      "Eval num_timesteps=4880000, episode_reward=-441.28 +/- 131.43\n",
      "Episode length: 1971.40 +/- 502.74\n",
      "Eval num_timesteps=4885000, episode_reward=-287.07 +/- 8.32\n",
      "Episode length: 1134.00 +/- 332.33\n",
      "Eval num_timesteps=4890000, episode_reward=-294.36 +/- 10.11\n",
      "Episode length: 1877.60 +/- 447.87\n",
      "Eval num_timesteps=4895000, episode_reward=-305.24 +/- 4.98\n",
      "Episode length: 1332.80 +/- 446.20\n",
      "Eval num_timesteps=4900000, episode_reward=-299.20 +/- 6.51\n",
      "Episode length: 1334.40 +/- 459.37\n",
      "Eval num_timesteps=4905000, episode_reward=-304.56 +/- 6.23\n",
      "Episode length: 1159.80 +/- 252.46\n",
      "Eval num_timesteps=4910000, episode_reward=-311.38 +/- 2.61\n",
      "Episode length: 1191.80 +/- 276.47\n",
      "Eval num_timesteps=4915000, episode_reward=-308.75 +/- 8.24\n",
      "Episode length: 1106.80 +/- 343.06\n",
      "Eval num_timesteps=4920000, episode_reward=-303.22 +/- 10.94\n",
      "Episode length: 1072.40 +/- 236.04\n",
      "Eval num_timesteps=4925000, episode_reward=-308.79 +/- 7.50\n",
      "Episode length: 1261.40 +/- 180.78\n",
      "Eval num_timesteps=4930000, episode_reward=-318.69 +/- 5.75\n",
      "Episode length: 1188.80 +/- 265.53\n",
      "Eval num_timesteps=4935000, episode_reward=-84.86 +/- 29.87\n",
      "Episode length: 3000.00 +/- 0.00\n",
      "Eval num_timesteps=4940000, episode_reward=-230.47 +/- 129.43\n",
      "Episode length: 2377.40 +/- 656.06\n",
      "Eval num_timesteps=4945000, episode_reward=-306.54 +/- 100.34\n",
      "Episode length: 1944.00 +/- 641.26\n",
      "Eval num_timesteps=4950000, episode_reward=-360.53 +/- 165.31\n",
      "Episode length: 1614.20 +/- 714.53\n",
      "Eval num_timesteps=4955000, episode_reward=-428.99 +/- 30.79\n",
      "Episode length: 1238.20 +/- 256.53\n",
      "Eval num_timesteps=4960000, episode_reward=-437.85 +/- 188.64\n",
      "Episode length: 2309.40 +/- 459.81\n",
      "Eval num_timesteps=4965000, episode_reward=-596.12 +/- 91.97\n",
      "Episode length: 1536.00 +/- 390.81\n",
      "Eval num_timesteps=4970000, episode_reward=-663.05 +/- 74.22\n",
      "Episode length: 1188.80 +/- 341.66\n",
      "Eval num_timesteps=4975000, episode_reward=-666.98 +/- 76.36\n",
      "Episode length: 1649.80 +/- 594.41\n",
      "Eval num_timesteps=4980000, episode_reward=-596.92 +/- 86.38\n",
      "Episode length: 1485.80 +/- 598.28\n",
      "Eval num_timesteps=4985000, episode_reward=-568.09 +/- 100.48\n",
      "Episode length: 1687.80 +/- 863.15\n",
      "Eval num_timesteps=4990000, episode_reward=-631.70 +/- 53.41\n",
      "Episode length: 1127.20 +/- 302.21\n",
      "Eval num_timesteps=4995000, episode_reward=-739.85 +/- 10.67\n",
      "Episode length: 1072.60 +/- 412.31\n",
      "Eval num_timesteps=5000000, episode_reward=-564.62 +/- 105.79\n",
      "Episode length: 965.00 +/- 285.62\n"
     ]
    }
   ],
   "source": [
    "trainer.train(total_timesteps=5_000_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
